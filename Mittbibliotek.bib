
@book{allaire_rmarkdown_2020,
	title = {Rmarkdown: {Dynamic} {Documents} for r},
	author = {Allaire, JJ and Xie, Yihui and McPherson, Jonathan and Luraschi, Javier and Ushey, Kevin and Atkins, Aron and Wickham, Hadley and Cheng, Joe and Chang, Winston and Iannone, Richard},
	year = {2020},
	keywords = {done},
	annote = {R package version 2.2},
}

@article{barnes_publish_2010,
	title = {Publish {Your} {Computer} {Code}: {It} {Is} {Good} {Enough}},
	volume = {467},
	issn = {0028-0836, 1476-4687},
	shorttitle = {Publish {Your} {Computer} {Code}},
	doi = {10.1038/467753a},
	language = {en},
	number = {7317},
	journal = {Nature},
	author = {Barnes, Nick},
	month = oct,
	year = {2010},
	pages = {753--753},
}

@article{bechhofer_why_2013,
	title = {Why {Linked} {Data} {Is} {Not} {Enough} for {Scientists}},
	volume = {29},
	issn = {0167739X},
	doi = {10.1016/j.future.2011.08.004},
	language = {en},
	number = {2},
	journal = {Future Generation Computer Systems},
	author = {Bechhofer, Sean and Buchan, Iain and De Roure, David and Missier, Paolo and Ainsworth, John and Bhagat, Jiten and Couch, Philip and Cruickshank, Don and Delderfield, Mark and Dunlop, Ian and Gamble, Matthew and Michaelides, Danius and Owen, Stuart and Newman, David and Sufi, Shoaib and Goble, Carole},
	month = feb,
	year = {2013},
	keywords = {done},
	pages = {599--611},
}

@techreport{bollen_social_2015,
	title = {Social, {Behavioral}, and {Economic} {Sciences} {Perspectives} on {Robust} and {Reliable} {Science}},
	number = {Report of the Subcommittee on Replicability in Science Advisory Committee to the National Science Foundation Directorate for Social, Behavioral, and Economic Sciences},
	institution = {NSF},
	author = {Bollen, Kenneth and Cacioppo, John T. and Krosnick, Jon A. and Olds, James L. and Kaplan, Robert M.},
	year = {2015},
	keywords = {done},
}

@inproceedings{brase_datacite_2009,
	address = {Beijing, China},
	title = {{DataCite} - {A} {Global} {Registration} {Agency} for {Research} {Data}},
	isbn = {978-0-7695-3898-3},
	doi = {10.1109/COINFO.2009.66},
	booktitle = {2009 {Fourth} {International} {Conference} on {Cooperation} and {Promotion} of {Information} {Resources} in {Science} and {Technology}},
	publisher = {IEEE},
	author = {Brase, Jan},
	month = nov,
	year = {2009},
	keywords = {done},
	pages = {257--261},
}

@incollection{buckheit_wavelab_1995,
	address = {New York, NY},
	title = {{WaveLab} and {Reproducible} {Research}},
	volume = {103},
	isbn = {978-0-387-94564-4 978-1-4612-2544-7},
	abstract = {WaveLab is a library of Matlab routines for wavelet analysis, wavelet-packet analysis, cosine-packet analysis and matching pursuit. The library is available free of charge over the Internet. Versions are provided for Macintosh, UNIX and Windows machines.},
	language = {en},
	booktitle = {Wavelets and {Statistics}},
	publisher = {Springer New York},
	author = {Buckheit, Jonathan B. and Donoho, David L.},
	editor = {Bickel, P. and Diggle, P. and Fienberg, S. and Krickeberg, K. and Olkin, I. and Wermuth, N. and Zeger, S. and Antoniadis, Anestis and Oppenheim, Georges},
	year = {1995},
	doi = {10.1007/978-1-4612-2544-7_5},
	keywords = {done},
	pages = {55--81},
}

@misc{conyers_learn_nodate,
	title = {Learn {Git} in 3 {Hours}},
	url = {https://www.safaribooksonline.com/videos/learn-git-in/9781789348231/9781789348231-video1_2},
	abstract = {Build powerful and effective projects using Git Version Control Systems About This Video Learn how to create, contribute to, and collaborate on software projects using Git Understand its fundamental features,...},
	author = {Conyers, Ross},
	note = {Publication Title: O'Reilly {\textbar} Safari},
}

@article{dewald_replication_1986,
	title = {Replication in {Empirical} {Economics}: {The} {Journal} of {Money}, {Credit} and {Banking} {Project}},
	volume = {76},
	issn = {0002-8282},
	shorttitle = {Replication in {Empirical} {Economics}},
	abstract = {This paper examines the role of replication in empirical economic research. It presents the findings of a two-year study that collected programs and data from authors and attempted to replicate their published results. Our research provides new and important information about the extent and causes of failures to replicate published results in economics. Our findings suggest that inadvertent errors in published empirical articles are a commonplace rather thana rare occurrence.},
	number = {4},
	journal = {The American Economic Review},
	author = {Dewald, William G. and Thursby, Jerry G. and Anderson, Richard G.},
	year = {1986},
	note = {Publisher: American Economic Association},
	keywords = {done},
	pages = {587--603},
}

@article{dewald_replication_2020,
	title = {Replication in {Empirical} {Economics}: {The} {Journal} of {Money}, {Credit} and {Banking} {Project}},
	language = {en},
	author = {Dewald, William G and Thursby, Jerry G and Anderson, Richard G},
	year = {2020},
	keywords = {done},
	pages = {18},
}

@book{noauthor_access_2015,
	address = {LU},
	title = {Access to and {Preservation} of {Scientific} {Information} in {Europe}: {Report} on the {Implementation} of {Commission} {Recommendation} {C}(2012) 4890 {Final}.},
	shorttitle = {Access to and {Preservation} of {Scientific} {Information} in {Europe}},
	language = {eng},
	publisher = {Publications Office},
	year = {2015},
	keywords = {done},
}

@article{ezekiel_considerations_1933,
	title = {Some {Considerations} on the {Analysis} of the {Prices} of {Competing} or {Substitute} {Commodities}},
	volume = {1},
	issn = {00129682, 14680262},
	number = {2},
	journal = {Econometrica},
	author = {Ezekiel, Mordecai},
	year = {1933},
	note = {Publisher: [Wiley, Econometric Society]},
	keywords = {done},
	pages = {172--180},
}

@article{fomel_guest_2009,
	title = {Guest {Editors}' {Introduction}: {Reproducible} {Research}},
	volume = {11},
	number = {1},
	journal = {Computing in Science Engineering},
	author = {Fomel, S. and Claerbout, J. F.},
	year = {2009},
	keywords = {done},
	pages = {5--7},
}

@article{frisch_editors_1933,
	title = {Editor's {Note}},
	volume = {1},
	issn = {00129682, 14680262},
	number = {1},
	journal = {Econometrica},
	author = {Frisch, Ragnar},
	year = {1933},
	note = {Publisher: [Wiley, Econometric Society]},
	keywords = {done},
	pages = {1--4},
}

@article{gentleman_reproducible_2005,
	title = {Reproducible {Research}: {A} {Bioinformatics} {Case} {Study}},
	volume = {4},
	issn = {1544-6115, 2194-6302},
	shorttitle = {Reproducible {Research}},
	doi = {10.2202/1544-6115.1034},
	abstract = {While scientific research and the methodologies involved have gone through substantial technological evolution the technology involved in the publication of the results of these endeavors has remained relatively stagnant. Publication is largely done in the same manner today as it was fifty years ago. Many journals have adopted electronic formats, however, their orientation and style is little different from a printed document. The documents tend to be static and take little advantage of computational resources that might be available. Recent work, Gentleman and Temple Lang (2004), suggests a methodology and basic infrastructure that can be used to publish documents in a substantially different way. Their approach is suitable for the publication of papers whose message relies on computation. Stated quite simply, Gentleman and Temple Lang propose a paradigm where documents are mixtures of code and text. Such documents may be self-contained or they may be a component of a compendium which provides the infrastructure needed to provide access to data and supporting software. These documents, or compendiums, can be processed in a number of different ways. One transformation will be to replace the code with its output – thereby providing the familiar, but limited, static document.},
	language = {en},
	number = {1},
	journal = {Statistical Applications in Genetics and Molecular Biology},
	author = {Gentleman, Robert},
	month = jan,
	year = {2005},
	keywords = {done},
}

@article{gentleman_statistical_2007,
	title = {Statistical {Analyses} and {Reproducible} {Research}},
	volume = {16},
	doi = {10.1198/106186007X178663},
	abstract = {It is important, if not essential, to integrate the computations and code used in data analyses, methodological descriptions, simulations, and so on with the documents that describe and rely on them. This integration allows readers to both verify and adapt the claims in the documents. Authors can easily reproduce the results in the future, and they can present the document's contents in a different medium, for example, with interactive controls. This article describes a software framework for both authoring and distributing these integrated, dynamic documents that contain text, code, data, and any auxiliary content needed to recreate the computations. The documents are dynamic in that the contents— including figures, tables, and so on— can be recalculated each time a view of the document is generated. Our model treats a dynamic document as a master or “source” document from which one can generate different views in the form of traditional, derived documents for different audiences.We introduce the concept of a compendium as a container for one or more dynamic documents and the different elements needed when processing them, such as code and data. The compendium serves as a means for distributing, managing, and updating the collection.The step from disseminating analyses via a compendium to reproducible research is a small one. By reproducible research, we mean research papers with accompanying software tools that allow the reader to directly reproduce the results and employ the computational methods that are presented in the research paper. Some of the issues involved in paradigms for the production, distribution, and use of such reproducible research are discussed.},
	number = {1},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Gentleman, Robert and Lang, Duncan Temple},
	year = {2007},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/106186007X178663},
	keywords = {done},
	pages = {1--23},
}

@article{golub_molecular_1999,
	title = {Molecular {Classification} of {Cancer}: {Class} {Discovery} and {Class} {Prediction} by {Gene} {Expression} {Monitoring}},
	volume = {286},
	issn = {0036-8075},
	shorttitle = {Molecular {Classification} of {Cancer}},
	doi = {10.1126/science.286.5439.531},
	abstract = {Although cancer classification has improved over the past 30 years, there has been no general approach for identifying new cancer classes (class discovery) or for assigning tumors to known classes (class prediction). Here, a generic approach to cancer classification based on gene expression monitoring by DNA microarrays is described and applied to human acute leukemias as a test case. A class discovery procedure automatically discovered the distinction between acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL) without previous knowledge of these classes. An automatically derived class predictor was able to determine the class of new leukemia cases. The results demonstrate the feasibility of cancer classification based solely on gene expression monitoring and suggest a general strategy for discovering and predicting cancer classes for other types of cancer, independent of previous biological knowledge.},
	language = {eng},
	number = {5439},
	journal = {Science (New York, N.Y.)},
	author = {Golub, T. R. and Slonim, D. K. and Tamayo, P. and Huard, C. and Gaasenbeek, M. and Mesirov, J. P. and Coller, H. and Loh, M. L. and Downing, J. R. and Caligiuri, M. A. and Bloomfield, C. D. and Lander, E. S.},
	month = oct,
	year = {1999},
	pmid = {10521349},
	keywords = {done},
	pages = {531--537},
}

@article{goodman_what_2016,
	title = {What {Does} {Research} {Reproducibility} {Mean}?},
	volume = {8},
	issn = {1946-6234, 1946-6242},
	doi = {10.1126/scitranslmed.aaf5027},
	language = {en},
	number = {341},
	journal = {Science Translational Medicine},
	author = {Goodman, Steven N. and Fanelli, Daniele and Ioannidis, John P. A.},
	month = jun,
	year = {2016},
	keywords = {done},
	pages = {341ps12--341ps12},
}

@book{grolemund_r_nodate,
	title = {R for {Data} {Science}},
	abstract = {This book will teach you how to do data science with R: You'll learn how to get your data into R, get it into the most useful structure, transform it, visualise it and model it. In this book, you will find a practicum of skills for data science. Just as a chemist learns how to clean test tubes and stock a lab, you'll learn how to clean data and draw plots— and many other things besides. These are the skills that allow data science to happen, and here you will find the best practices for doing each of these things with R. You'll learn how to use the grammar of graphics, literate programming, and reproducible research to save time. You'll also learn how to manage cognitive resources to facilitate discoveries when wrangling, visualising, and exploring data.},
	author = {Grolemund, Garrett and Wickham, Hadley},
}

@misc{gruber_daring_nodate,
	title = {Daring {Fireball}: {Markdown}},
	url = {https://daringfireball.net/projects/markdown/},
	author = {Gruber, John and Swartz, Aron},
	keywords = {done},
}

@misc{gruber_daring_nodate-1,
	title = {Daring {Fireball}: {Markdown}},
	url = {https://daringfireball.net/projects/markdown/},
	author = {Gruber, John and Swartz, Aron},
	keywords = {done},
}

@article{ioannidis_why_2005,
	title = {Why {Most} {Published} {Research} {Findings} {Are} {False}},
	volume = {2},
	issn = {1549-1676},
	doi = {10.1371/journal.pmed.0020124},
	abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
	language = {en},
	number = {8},
	journal = {PLOS Medicine},
	author = {Ioannidis, John P. A.},
	month = aug,
	year = {2005},
	note = {Publisher: Public Library of Science},
	keywords = {done},
	pages = {e124},
}

@article{iyengar_selection_1988,
	title = {Selection {Models} and the {File} {Drawer} {Problem}},
	volume = {3},
	issn = {08834237},
	abstract = {Meta-analysis consists of quantitative methods for combining evidence from different studies about a particular issue. A frequent criticism of meta-analysis is that it may be based on a biased sample of all studies that were done. In this paper, we use selection models, or weighted distributions, to deal with one source of bias, namely, the failure to report studies that do not yield statistically significant results. We apply selection models to two approaches that have been suggested for correcting the bias. The fail-safe sample size approach calculates the minimum number of unpublished studies showing nonsignificant results that must have been carried out in order to overturn the conclusion reached from the published studies. The maximum likelihood approach uses a weighted distribution to model the selection bias in the generation of the data and estimates various parameters of interest. We suggest the use of families of weight functions to model plausible biasing mechanisms to study the sensitivity of inferences about effect sizes. By using an example, we show that the maximum likelihood approach has several advantages over the fail-safe sample size approach.},
	number = {1},
	journal = {Statistical Science},
	author = {Iyengar, Satish and Greenhouse, Joel B.},
	year = {1988},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {done},
	pages = {109--117},
}

@article{jasny_again_2011,
	title = {Again, and {Again}, and {Again}},
	volume = {334},
	issn = {0036-8075},
	doi = {10.1126/science.334.6060.1225},
	number = {6060},
	journal = {Science},
	author = {Jasny, Barbara R. and Chin, Gilbert and Chong, Lisa and Vignieri, Sacha},
	year = {2011},
	note = {Publisher: American Association for the Advancement of Science
\_eprint: https://science.sciencemag.org/content/334/6060/1225.full.pdf},
	keywords = {done},
	pages = {1225--1225},
}

@article{klein_many_2018,
	title = {Many {Labs} 2: {Investigating} {Variation} in {Replicability} across {Samples} and {Settings}},
	volume = {1},
	doi = {10.1177/2515245918810225},
	abstract = {We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p ¡ .05), we found that 15 (54\%) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p ¡ .0001), 14 (50\%) of the replications still provided such evidence, a reflection of the extremely high-powered design. Seven (25\%) of the replications yielded effect sizes larger than the original ones, and 21 (75\%) yielded effect sizes smaller than the original ones. The median comparable Cohen's ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small (¡ 0.20) in 16 of the replications (57\%), and 9 effects (32\%) were in the direction opposite the direction of the original effect. Across settings, the Q statistic indicated significant heterogeneity in 11 (39\%) of the replication effects, and most of those were among the findings with the largest overall effect sizes; only 1 effect that was near zero in the aggregate showed significant heterogeneity according to this measure. Only 1 effect had a tau value greater than .20, an indication of moderate heterogeneity. Eight others had tau values near or slightly above .10, an indication of slight heterogeneity. Moderation tests indicated that very little heterogeneity was attributable to the order in which the tasks were performed or whether the tasks were administered in lab versus online. Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized, rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness scores, respectively). Cumulatively, variability in the observed effect sizes was attributable more to the effect being studied than to the sample or setting in which it was studied.},
	number = {4},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Klein, Richard A. and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G. and Reginald B. Adams, Jr. and Alper, Sinan and Aveyard, Mark and Axt, Jordan R. and Babalola, Mayowa T. and Bahník, Štěpán and Batra, Rishtee and Berkics, Mihály and Bernstein, Michael J. and Berry, Daniel R. and Bialobrzeska, Olga and Binan, Evans Dami and Bocian, Konrad and Brandt, Mark J. and Busching, Robert and Rédei, Anna Cabak and Cai, Huajian and Cambier, Fanny and Cantarero, Katarzyna and Carmichael, Cheryl L. and Ceric, Francisco and Chandler, Jesse and Chang, Jen-Ho and Chatard, Armand and Chen, Eva E. and Cheong, Winnee and Cicero, David C. and Coen, Sharon and Coleman, Jennifer A. and Collisson, Brian and Conway, Morgan A. and Corker, Katherine S. and Curran, Paul G. and Cushman, Fiery and Dagona, Zubairu K. and Dalgar, Ilker and Rosa, Anna Dalla and Davis, William E. and de Bruijn, Maaike and Schutter, Leander De and Devos, Thierry and de Vries, Marieke and Doğulu, Canay and Dozo, Nerisa and Dukes, Kristin Nicole and Dunham, Yarrow and Durrheim, Kevin and Ebersole, Charles R. and Edlund, John E. and Eller, Anja and English, Alexander Scott and Finck, Carolyn and Frankowska, Natalia and Freyre, Miguel-Ángel and Friedman, Mike and Galliani, Elisa Maria and Gandi, Joshua C. and Ghoshal, Tanuka and Giessner, Steffen R. and Gill, Tripat and Gnambs, Timo and Gómez, Ángel and González, Roberto and Graham, Jesse and Grahe, Jon E. and Grahek, Ivan and Green, Eva G. T. and Hai, Kakul and Haigh, Matthew and Haines, Elizabeth L. and Hall, Michael P. and Heffernan, Marie E. and Hicks, Joshua A. and Houdek, Petr and Huntsinger, Jeffrey R. and Huynh, Ho Phi and IJzerman, Hans and Inbar, Yoel and Innes-Ker, Åse H. and Jiménez-Leal, William and John, Melissa-Sue and Joy-Gaba, Jennifer A. and Kamiloğlu, Roza G. and Kappes, Heather Barry and Karabati, Serdar and Karick, Haruna and Keller, Victor N. and Kende, Anna and Kervyn, Nicolas and Knežević, Goran and Kovacs, Carrie and Krueger, Lacy E. and Kurapov, German and Kurtz, Jamie and Lakens, Daniël and Lazarević, Ljiljana B. and Levitan, Carmel A. and Neil A. Lewis, Jr. and Lins, Samuel and Lipsey, Nikolette P. and Losee, Joy E. and Maassen, Esther and Maitner, Angela T. and Malingumu, Winfrida and Mallett, Robyn K. and Marotta, Satia A. and Me{\textbackslash}djedović, Janko and Mena-Pacheco, Fernando and Milfont, Taciano L. and Morris, Wendy L. and Murphy, Sean C. and Myachykov, Andriy and Neave, Nick and Neijenhuijs, Koen and Nelson, Anthony J. and Neto, Félix and Nichols, Austin Lee and Ocampo, Aaron and O'Donnell, Susan L. and Oikawa, Haruka and Oikawa, Masanori and Ong, Elsie and Orosz, Gábor and Osowiecka, Malgorzata and Packard, Grant and Pérez-Sánchez, Rolando and Petrović, Boban and Pilati, Ronaldo and Pinter, Brad and Podesta, Lysandra and Pogge, Gabrielle and Pollmann, Monique M. H. and Rutchick, Abraham M. and Saavedra, Patricio and Saeri, Alexander K. and Salomon, Erika and Schmidt, Kathleen and Schönbrodt, Felix D. and Sekerdej, Maciej B. and Sirlopú, David and Skorinko, Jeanine L. M. and Smith, Michael A. and Smith-Castro, Vanessa and Smolders, Karin C. H. J. and Sobkow, Agata and Sowden, Walter and Spachtholz, Philipp and Srivastava, Manini and Steiner, Troy G. and Stouten, Jeroen and Street, Chris N. H. and Sundfelt, Oskar K. and Szeto, Stephanie and Szumowska, Ewa and Tang, Andrew C. W. and Tanzer, Norbert and Tear, Morgan J. and Theriault, Jordan and Thomae, Manuela and Torres, David and Traczyk, Jakub and Tybur, Joshua M. and Ujhelyi, Adrienn and van Aert, Robbie C. M. and van Assen, Marcel A. L. M. and van der Hulst, Marije and van Lange, Paul A. M. and van 't Veer, Anna Elisabeth and Echeverría, Alejandro Vásquez- and Vaughn, Leigh Ann and Vázquez, Alexandra and Vega, Luis Diego and Verniers, Catherine and Verschoor, Mark and Voermans, Ingrid P. J. and Vranka, Marek A. and Welch, Cheryl and Wichman, Aaron L. and Williams, Lisa A. and Wood, Michael and Woodzicka, Julie A. and Wronska, Marta K. and Young, Liane and Zelenski, John M. and Zhijia, Zeng and Nosek, Brian A.},
	year = {2018},
	note = {\_eprint: https://doi.org/10.1177/2515245918810225},
	keywords = {done},
	pages = {443--490},
}

@article{knuth_literate_1984,
	title = {Literate {Programming}},
	volume = {27},
	issn = {0010-4620},
	doi = {10.1093/comjnl/27.2.97},
	abstract = {The author and his associates have been experimenting for the past several years with a programming language and documentation system called WEB. This paper presents WEB by example, and discusses why the new system appears to be an improvement over previous ones.},
	number = {2},
	journal = {The Computer Journal},
	author = {Knuth, D. E.},
	month = jan,
	year = {1984},
	note = {\_eprint: https://academic.oup.com/comjnl/article-pdf/27/2/97/981657/270097.pdf},
	keywords = {done},
	pages = {97--111},
}

@book{knuth_texbook_1986,
	series = {Computers \& {Typesetting}},
	title = {The {TeXbook}},
	isbn = {978-0-201-13447-6},
	publisher = {Addison-Wesley},
	author = {Knuth, D.E.},
	year = {1986},
	lccn = {85030845},
	keywords = {done},
}

@book{knuth_literate_1992,
	title = {Literate {Programming}},
	isbn = {978-0-937073-81-0},
	abstract = {Literate programming is a programming methodology that combines a programming language with a documentation language, making programs more robust, more portable, and more easily maintained than programs written only in a high-level language. Computer programmers already know both kinds of languages; they need only learn a few conventions about alternating between languages to create programs that are works of literature. A literate programmer is an essayist who writes programs for humans to understand, instead of primarily writing instructions for machines to follow. When programs are written in the recommended style they can be transformed into documents by a document compiler and into efficient code by an algebraic compiler. This anthology of essays from the inventor of literate programming includes Knuth's early papers on related topics such as structured programming, as well as the Computer Journal article that launched literate programming itself.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Knuth, Donald E.},
	month = mar,
	year = {1992},
	keywords = {done},
}

@book{lander_r_2017,
	address = {Boston},
	edition = {2nd Edition},
	title = {R for {Everyone}: {Advanced} {Analytics} and {Graphics}},
	isbn = {978-0-13-454692-6},
	shorttitle = {R for {Everyone}},
	abstract = {Statistical Computation for Programmers, Scientists, Quants, Excel Users, and Other Professionals Using the open source R language, you can build powerful statistical models to answer many of your most challenging questions. R has traditionally been difficult for non-statisticians to learn, and most R books assume far too much knowledge to be of help. R for Everyone, Second Edition, is the solution. Drawing on his unsurpassed experience teaching new users, professional data scientist Jared P. Lander has written the perfect tutorial for anyone new to statistical programming and modeling. Organized to make learning easy and intuitive, this guide focuses on the 20 percent of R functionality you'll need to accomplish 80 percent of modern data tasks. Lander's self-contained chapters start with the absolute basics, offering extensive hands-on practice and sample code. You'll download and install R; navigate and use the R environment; master basic program control, data import, manipulation, and visualization; and walk through several essential tests. Then, building on this foundation, you'll construct several complete models, both linear and nonlinear, and use some data mining techniques. After all this you'll make your code reproducible with LaTeX, RMarkdown, and Shiny. By the time you're done, you won't just know how to write R programs, you'll be ready to tackle the statistical problems you care about most. Coverage includes Explore R, RStudio, and R packages Use R for math: variable types, vectors, calling functions, and more Exploit data structures, including data.frames, matrices, and lists Read many different types of data Create attractive, intuitive statistical graphics Write user-defined functions Control program flow with if, ifelse, and complex checks Improve program efficiency with group manipulations Combine and reshape multiple datasets Manipulate strings using R's facilities and regular expressions Create normal, binomial, and Poisson probability distributions Build linear, generalized linear, and nonlinear models Program basic statistics: mean, standard deviation, and t-tests Train machine learning models Assess the quality of models and variable selection Prevent overfitting and perform variable selection, using the Elastic Net and Bayesian methods Analyze univariate and multivariate time series data Group data via K-means and hierarchical clustering Prepare reports, slideshows, and web pages with knitr Display interactive data with RMarkdown and htmlwidgets Implement dashboards with Shiny Build reusable R packages with devtools and Rcpp Register your product at informit.com/register for convenient access to downloads, updates, and corrections as they become available.},
	language = {English},
	publisher = {Addison-Wesley Professional},
	author = {Lander, Jared P.},
	month = jun,
	year = {2017},
}

@incollection{leisch_sweave_2002,
	address = {Heidelberg},
	title = {Sweave: {Dynamic} {Generation} of {Statistical} {Reports} {Using} {Literate} {Data} {Analysis}},
	isbn = {978-3-7908-1517-7 978-3-642-57489-4},
	shorttitle = {Sweave},
	abstract = {Sweave combines typesetting with LATEX and data anlysis with S into integrated statistical documents. When run through R or Splus, all data analysis output (tables, graphs, . . . ) is created on the fly and inserted into a final LATEX document. Options control which parts of the original S code are shown to or hidden from the reader, respectively. Many S users are also LATEX users, hence no new software has to be learned. The report can be automatically updated if data or analysis change, which allows for truly reproducible research.},
	language = {en},
	booktitle = {Compstat},
	publisher = {Physica-Verlag HD},
	author = {Leisch, Friedrich},
	editor = {Härdle, Wolfgang and Rönz, Bernd},
	year = {2002},
	doi = {10.1007/978-3-642-57489-4_89},
	keywords = {done},
	pages = {575--580},
}

@article{markowetz_five_2015,
	title = {Five {Selfish} {Reasons} to {Work} {Reproducibly}},
	volume = {16},
	issn = {1474-760X},
	doi = {10.1186/s13059-015-0850-7},
	language = {en},
	number = {1},
	journal = {Genome Biology},
	author = {Markowetz, Florian},
	month = dec,
	year = {2015},
	pages = {274},
}

@article{mccullough_economics_2008,
	title = {Do {Economics} {Journal} {Archives} {Promote} {Replicable} {Research}?},
	volume = {41},
	copyright = {© Canadian Economics Association},
	issn = {1540-5982},
	doi = {10.1111/j.1540-5982.2008.00509.x},
	abstract = {Abstract. All the long-standing archives at economics journals do not facilitate the reproduction of published results. The data-only archives at Journal of Business and Economic Statistics and Economic Journal fail in part because most authors do not contribute data. Results published in the FRB St. Louis Review can rarely be reproduced using the data+code in the journal archive. Recently created archives at top journals should avoid the mistakes of their predecessors. We categorize reasons for archives' failures and identify successful policies.},
	language = {en},
	number = {4},
	journal = {Canadian Journal of Economics/Revue canadienne d'économique},
	author = {McCullough, B. D. and McGeary, Kerry Anne and Harrison, Teresa D.},
	year = {2008},
	keywords = {done},
	pages = {1406--1420},
}

@article{mcnutt_reproducibility_2014,
	title = {Reproducibility},
	volume = {343},
	issn = {0036-8075},
	doi = {10.1126/science.1250475},
	number = {6168},
	journal = {Science},
	author = {McNutt, Marcia},
	year = {2014},
	note = {Publisher: American Association for the Advancement of Science
\_eprint: https://science.sciencemag.org/content/343/6168/229.full.pdf},
	keywords = {done},
	pages = {229--229},
}

@article{nosek_promoting_2015,
	title = {Promoting an {Open} {Research} {Culture}},
	volume = {348},
	issn = {0036-8075, 1095-9203},
	doi = {10.1126/science.aab2374},
	language = {en},
	number = {6242},
	journal = {Science},
	author = {Nosek, B. A. and Alter, G. and Banks, G. C. and Borsboom, D. and Bowman, S. D. and Breckler, S. J. and Buck, S. and Chambers, C. D. and Chin, G. and Christensen, G. and Contestabile, M. and Dafoe, A. and Eich, E. and Freese, J. and Glennerster, R. and Goroff, D. and Green, D. P. and Hesse, B. and Humphreys, M. and Ishiyama, J. and Karlan, D. and Kraut, A. and Lupia, A. and Mabry, P. and Madon, T. and Malhotra, N. and Mayo-Wilson, E. and McNutt, M. and Miguel, E. and Paluck, E. L. and Simonsohn, U. and Soderberg, C. and Spellman, B. A. and Turitto, J. and VandenBos, G. and Vazire, S. and Wagenmakers, E. J. and Wilson, R. and Yarkoni, T.},
	month = jun,
	year = {2015},
	keywords = {done},
	pages = {1422--1425},
}

@article{nust_opening_2017,
	title = {Opening the {Publication} {Process} with {Executable} {Research} {Compendia}},
	volume = {23},
	issn = {1082-9873},
	doi = {10.1045/january2017-nuest},
	language = {en},
	number = {1/2},
	journal = {D-Lib Magazine},
	author = {Nüst, Daniel and Konkol, Markus and Pebesma, Edzer and Kray, Christian and Schutzeichel, Marc and Przibytzin, Holger and Lorenz, Jörg},
	month = jan,
	year = {2017},
}

@article{collaboration_estimating_2015,
	title = {Estimating the {Reproducibility} of {Psychological} {Science}},
	volume = {349},
	issn = {0036-8075},
	doi = {10.1126/science.aac4716},
	abstract = {One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.Science, this issue 10.1126/science.aac4716INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P \&lt; .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that we already know this belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
	number = {6251},
	journal = {Science},
	author = {Collaboration, Open Science},
	year = {2015},
	note = {Publisher: American Association for the Advancement of Science
\_eprint: https://science.sciencemag.org/content/349/6251/aac4716.full.pdf},
	keywords = {done},
}

@article{peng_reproducible_2011,
	title = {Reproducible {Research} in {Computational} {Science}},
	volume = {334},
	copyright = {Copyright © 2011, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	doi = {10.1126/science.1213847},
	abstract = {Computational science has led to exciting new developments, but the nature of the work has exposed limitations in our ability to evaluate published findings. Reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible.},
	language = {en},
	number = {6060},
	journal = {Science},
	author = {Peng, Roger D.},
	month = dec,
	year = {2011},
	pmid = {22144613},
	note = {Publisher: American Association for the Advancement of Science
Section: Perspective},
	keywords = {done},
	pages = {1226--1227},
}

@article{ram_git_2013,
	title = {Git {Can} {Facilitate} {Greater} {Reproducibility} and {Increased} {Transparency} in {Science}},
	volume = {8},
	issn = {1751-0473},
	doi = {10.1186/1751-0473-8-7},
	abstract = {Reproducibility is the hallmark of good science. Maintaining a high degree of transparency in scientific reporting is essential not just for gaining trust and credibility within the scientific community but also for facilitating the development of new ideas. Sharing data and computer code associated with publications is becoming increasingly common, motivated partly in response to data deposition requirements from journals and mandates from funders. Despite this increase in transparency, it is still difficult to reproduce or build upon the findings of most scientific publications without access to a more complete workflow.},
	number = {1},
	journal = {Source Code for Biology and Medicine},
	author = {Ram, Karthik},
	month = feb,
	year = {2013},
	pages = {7},
}

@misc{ramsey_noweb_nodate,
	title = {Noweb {Home} {Page}},
	url = {https://www.cs.tufts.edu/\textasciitilde nr/noweb/},
	author = {Ramsey, Norman},
	keywords = {done},
}

@book{r_core_team_r_2020,
	address = {Vienna, Austria},
	title = {R: {A} {Language} and {Environment} for {Statistical} {Computing}},
	publisher = {R Foundation for Statistical Computing},
	author = {{R Core Team}},
	year = {2020},
	keywords = {done},
}

@book{riederer_r_nodate,
	title = {R {Markdown} {Cookbook}},
	abstract = {This book showcases short, practical examples of lesser-known tips and tricks to helps users get the most out of these tools. After reading this book, you will understand how R Markdown documents are transformed from plain text and how you may customize nearly every step of this processing. For example, you will learn how to dynamically create content from R code, reference code in other documents or chunks, control the formatting with customer templates, fine-tune how your code is processed, and incorporate multiple languages into your analysis.},
	author = {Riederer, Christophe Dervieux, Emily, Yihui Xie},
	keywords = {done},
}

@inproceedings{rosenthal_file_1979,
	title = {The {File} {Drawer} {Problem} and {Tolerance} for {Null} {Results}.},
	volume = {86},
	publisher = {Psychological Bulletin},
	author = {Rosenthal, R.},
	year = {1979},
	keywords = {done},
	pages = {638--641},
}

@book{rstudio_team_rstudio_2020,
	address = {Boston, MA},
	title = {{RStudio}: {Integrated} {Development} {Environment} for r},
	publisher = {RStudio, PBC.},
	author = {{RStudio Team}},
	year = {2020},
}

@article{schwab_reproducible_1995,
	title = {Reproducible {Electronic} {Documents}},
	abstract = {To organize computational scientific research and hence to conveniently transfer our technology, we impose a simple filing discipline on the authors in our laboratory. A document's makefile includes laboratory-wide standard rules that offer readers these four standard commands: make burn removes the document's result figures, make build recomputes them, make view displays the figures, and make clean removes any intermediate files. Although we developed these standards to aid readers we discovered that authors are often the principal beneficiaries.},
	language = {en},
	author = {Schwab, Matthias and Karrenbach, Martin and Claerbout, Jon},
	year = {1995},
	keywords = {done},
	pages = {14},
}

@article{simmons_false-positive_2011,
	title = {False-{Positive} {Psychology}: {Undisclosed} {Flexibility} in {Data} {Collection} and {Analysis} {Allows} {Presenting} {Anything} as {Significant}},
	volume = {22},
	doi = {10.1177/0956797611417632},
	abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings (\${\textbackslash}leq\$ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
	number = {11},
	journal = {Psychological Science},
	author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
	year = {2011},
	pmid = {22006061},
	note = {\_eprint: https://doi.org/10.1177/0956797611417632},
	keywords = {done},
	pages = {1359--1366},
}

@book{tierney_rmarkdown_nodate,
	title = {{RMarkdown} for {Scientists}},
	abstract = {A book created for a 3 hour workshop on rmarkdown},
	author = {Tierney, Nicholas},
	keywords = {done},
}

@book{wickham_r_2016,
	address = {Beijing},
	title = {R for {Data} {Science}: {Import}, {Tidy}, {Transform}, {Visualize}, and {Model} {Data}},
	isbn = {978-1-4919-1039-9},
	shorttitle = {R for {Data} {Science}},
	language = {eng},
	publisher = {O'Reilly},
	author = {Wickham, Hadley and Grolemund, Garrett},
	year = {2016},
	lccn = {006.312 Wic, 519.2 W63r, 006.312 WIC},
}

@article{wikipedia_meta-analysis_2020,
	title = {Meta-{Analysis}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	abstract = {A meta-analysis is a statistical analysis that combines the results of multiple scientific studies. Meta-analysis can be performed when there are multiple scientific studies addressing the same question, with each individual study reporting measurements that are expected to have some degree of error. The aim then is to use approaches from statistics to derive a pooled estimate closest to the unknown common truth based on how this error is perceived. Existing methods for meta-analysis yield a weighted average from the results of the individual studies, and what differs is the manner in which these weights are allocated and also the manner in which the uncertainty is computed around the point estimate thus generated. In addition to providing an estimate of the unknown common truth, meta-analysis has the capacity to contrast results from different studies and identify patterns among study results, sources of disagreement among those results, or other interesting relationships that may come to light in the context of multiple studies.A key benefit of this approach is the aggregation of information leading to a higher statistical power and more robust point estimate than is possible from the measure derived from any individual study. However, in performing a meta-analysis, an investigator must make choices which can affect the results, including deciding how to search for studies, selecting studies based on a set of objective criteria, dealing with incomplete data, analyzing the data, and accounting for or choosing not to account for publication bias. Judgment calls made in completing a meta-analysis may affect the results. For example, Wanous and colleagues examined four pairs of meta-analyses on the four topics of (a) job performance and satisfaction relationship, (b) realistic job previews, (c) correlates of role conflict and ambiguity, and (d) the job satisfaction and absenteeism relationship, and illustrated how various judgement calls made by the researchers produced different results.Meta-analyses are often, but not always, important components of a systematic review procedure. For instance, a meta-analysis may be conducted on several clinical trials of a medical treatment, in an effort to obtain a better understanding of how well the treatment works. Here it is convenient to follow the terminology used by the Cochrane Collaboration, and use "meta-analysis" to refer to statistical methods of combining evidence, leaving other aspects of 'research synthesis' or 'evidence synthesis', such as combining information from qualitative studies, for the more general context of systematic reviews. A meta-analysis is a secondary source.},
	language = {en},
	journal = {Wikipedia},
	author = {{wikipedia}},
	month = aug,
	year = {2020},
	keywords = {done},
}

@incollection{xie_knitr_2014,
	title = {Knitr: {A} {Comprehensive} {Tool} for {Reproducible} {Research} in {R}},
	booktitle = {Implementing {Reproducible} {Computational} {Research}},
	publisher = {Chapman and Hall/CRC},
	author = {Xie, Yihui},
	editor = {Stodden, Victoria and Leisch, Friedrich and Peng, Roger D.},
	year = {2014},
	keywords = {done},
	annote = {ISBN 978-1466561595},
}

@book{xie_dynamic_2015,
	address = {Boca Raton, Florida},
	edition = {Second},
	title = {Dynamic {Documents} with {R} and {Knitr}},
	publisher = {Chapman and Hall/CRC},
	author = {Xie, Yihui},
	year = {2015},
	keywords = {done},
	annote = {ISBN 978-1498716963},
}

@book{xie_r_2018,
	address = {Boca Raton, Florida},
	title = {R {Markdown}: {The} {Definitive} {Guide}},
	publisher = {Chapman and Hall/CRC},
	author = {Xie, Yihui and Allaire, J.J. and Grolemund, Garrett},
	year = {2018},
	keywords = {done},
	annote = {ISBN 9781138359338},
}

@book{xie_knitr_2020,
	title = {Knitr: {A} {General}-{Purpose} {Package} for {Dynamic} {Report} {Generation} in r},
	author = {Xie, Yihui},
	year = {2020},
	note = {Type: Manual},
	keywords = {done},
	annote = {R package version 1.29},
}

@article{young_why_2008,
	title = {Why {Current} {Publication} {Practices} {May} {Distort} {Science}},
	volume = {5},
	issn = {1549-1676},
	doi = {10.1371/journal.pmed.0050201},
	abstract = {The current system of publication in biomedical research provides a distorted view of the reality of scientific data that are generated in the laboratory and clinic. This system can be studied by applying principles from the field of economics. The “winner's curse,” a more general statement of publication bias, suggests that the small proportion of results chosen for publication are unrepresentative of scientists' repeated samplings of the real world. The self-correcting mechanism in science is retarded by the extreme imbalance between the abundance of supply (the output of basic science laboratories and clinical investigations) and the increasingly limited venues for publication (journals with sufficiently high impact). This system would be expected intrinsically to lead to the misallocation of resources. The scarcity of available outlets is artificial, based on the costs of printing in an electronic age and a belief that selectivity is equivalent to quality. Science is subject to great uncertainty: we cannot be confident now which efforts will ultimately yield worthwhile achievements. However, the current system abdicates to a small number of intermediates an authoritative prescience to anticipate a highly unpredictable future. In considering society's expectations and our own goals as scientists, we believe that there is a moral imperative to reconsider how scientific data are judged and disseminated.},
	language = {en},
	number = {10},
	journal = {PLoS Medicine},
	author = {Young, Neal S and Ioannidis, John P. A and Al-Ubaydli, Omar},
	month = oct,
	year = {2008},
	keywords = {done},
	pages = {e201},
}

@misc{noauthor_dear_nodate,
	title = {Dear {Colleague} {Letter}: {Robust} and {Reliable} {Research} in the {Social}, {Behavioral}, and {Economic} {Sciences} ({Nsf16137}) {\textbar} {NSF} - {National} {Science} {Foundation}},
	url = {https://www.nsf.gov/pubs/2016/nsf16137/nsf16137.jsp},
	keywords = {done},
}

@misc{noauthor_dear_nodate-1,
	title = {Dear {Colleague} {Letter}: {Robust} and {Reliable} {Research} in the {Social}, {Behavioral}, and {Economic} {Sciences} ({Nsf16137}) {\textbar} {NSF} - {National} {Science} {Foundation}},
	url = {https://www.nsf.gov/pubs/2016/nsf16137/nsf16137.jsp},
	keywords = {done},
}

@misc{noauthor_american_nodate,
	title = {American {Economic} {Association}},
	url = {https://www.aeaweb.org/journals/data},
}

@misc{noauthor_pandoc_nodate,
	title = {Pandoc - {Pandoc} {User}'s {Guide}},
	url = {https://pandoc.org/MANUAL.html},
}

@article{nosek_estimating_2015,
	title = {Estimating the {Reproducibility} of {Psychological} {Science}},
	volume = {349},
	issn = {0036-8075},
	doi = {10.1126/science.aac4716},
	abstract = {One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.Science, this issue 10.1126/science.aac4716INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P \&lt; .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that we already know this belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
	number = {6251},
	journal = {Science},
	author = {Nosek, Brian A. and {et al}},
	year = {2015},
	note = {Publisher: American Association for the Advancement of Science
\_eprint: https://science.sciencemag.org/content/349/6251/aac4716.full.pdf},
	keywords = {done},
}

@book{allaire_rmarkdown_2020-1,
	title = {Rmarkdown: {Dynamic} {Documents} for r},
	author = {Allaire, JJ and Xie, Yihui and McPherson, Jonathan and Luraschi, Javier and Ushey, Kevin and Atkins, Aron and Wickham, Hadley and Cheng, Joe and Chang, Winston and Iannone, Richard},
	year = {2020},
	keywords = {done},
	annote = {R package version 2.2},
}

@article{barnes_publish_2010-1,
	title = {Publish {Your} {Computer} {Code}: {It} {Is} {Good} {Enough}},
	volume = {467},
	issn = {0028-0836, 1476-4687},
	shorttitle = {Publish {Your} {Computer} {Code}},
	doi = {10.1038/467753a},
	language = {en},
	number = {7317},
	journal = {Nature},
	author = {Barnes, Nick},
	month = oct,
	year = {2010},
	pages = {753--753},
}

@article{bechhofer_why_2013-1,
	title = {Why {Linked} {Data} {Is} {Not} {Enough} for {Scientists}},
	volume = {29},
	issn = {0167739X},
	doi = {10.1016/j.future.2011.08.004},
	language = {en},
	number = {2},
	journal = {Future Generation Computer Systems},
	author = {Bechhofer, Sean and Buchan, Iain and De Roure, David and Missier, Paolo and Ainsworth, John and Bhagat, Jiten and Couch, Philip and Cruickshank, Don and Delderfield, Mark and Dunlop, Ian and Gamble, Matthew and Michaelides, Danius and Owen, Stuart and Newman, David and Sufi, Shoaib and Goble, Carole},
	month = feb,
	year = {2013},
	keywords = {done},
	pages = {599--611},
}

@techreport{bollen_social_2015-1,
	title = {Social, {Behavioral}, and {Economic} {Sciences} {Perspectives} on {Robust} and {Reliable} {Science}},
	number = {Report of the Subcommittee on Replicability in Science Advisory Committee to the National Science Foundation Directorate for Social, Behavioral, and Economic Sciences},
	institution = {NSF},
	author = {Bollen, Kenneth and Cacioppo, John T. and Krosnick, Jon A. and Olds, James L. and Kaplan, Robert M.},
	year = {2015},
	keywords = {done},
}

@inproceedings{brase_datacite_2009-1,
	address = {Beijing, China},
	title = {{DataCite} - {A} {Global} {Registration} {Agency} for {Research} {Data}},
	isbn = {978-0-7695-3898-3},
	doi = {10.1109/COINFO.2009.66},
	booktitle = {2009 {Fourth} {International} {Conference} on {Cooperation} and {Promotion} of {Information} {Resources} in {Science} and {Technology}},
	publisher = {IEEE},
	author = {Brase, Jan},
	month = nov,
	year = {2009},
	keywords = {done},
	pages = {257--261},
}

@incollection{buckheit_wavelab_1995-1,
	address = {New York, NY},
	title = {{WaveLab} and {Reproducible} {Research}},
	volume = {103},
	isbn = {978-0-387-94564-4 978-1-4612-2544-7},
	abstract = {WaveLab is a library of Matlab routines for wavelet analysis, wavelet-packet analysis, cosine-packet analysis and matching pursuit. The library is available free of charge over the Internet. Versions are provided for Macintosh, UNIX and Windows machines.},
	language = {en},
	booktitle = {Wavelets and {Statistics}},
	publisher = {Springer New York},
	author = {Buckheit, Jonathan B. and Donoho, David L.},
	editor = {Bickel, P. and Diggle, P. and Fienberg, S. and Krickeberg, K. and Olkin, I. and Wermuth, N. and Zeger, S. and Antoniadis, Anestis and Oppenheim, Georges},
	year = {1995},
	doi = {10.1007/978-1-4612-2544-7_5},
	keywords = {done},
	pages = {55--81},
}

@misc{conyers_learn_nodate-1,
	title = {Learn {Git} in 3 {Hours}},
	url = {https://www.safaribooksonline.com/videos/learn-git-in/9781789348231/9781789348231-video1_2},
	abstract = {Build powerful and effective projects using Git Version Control Systems About This Video Learn how to create, contribute to, and collaborate on software projects using Git Understand its fundamental features,...},
	author = {Conyers, Ross},
	note = {Publication Title: O'Reilly {\textbar} Safari},
}

@article{dewald_replication_1986-1,
	title = {Replication in {Empirical} {Economics}: {The} {Journal} of {Money}, {Credit} and {Banking} {Project}},
	volume = {76},
	issn = {0002-8282},
	shorttitle = {Replication in {Empirical} {Economics}},
	abstract = {This paper examines the role of replication in empirical economic research. It presents the findings of a two-year study that collected programs and data from authors and attempted to replicate their published results. Our research provides new and important information about the extent and causes of failures to replicate published results in economics. Our findings suggest that inadvertent errors in published empirical articles are a commonplace rather thana rare occurrence.},
	number = {4},
	journal = {The American Economic Review},
	author = {Dewald, William G. and Thursby, Jerry G. and Anderson, Richard G.},
	year = {1986},
	note = {Publisher: American Economic Association},
	keywords = {done},
	pages = {587--603},
}

@article{dewald_replication_2020-1,
	title = {Replication in {Empirical} {Economics}: {The} {Journal} of {Money}, {Credit} and {Banking} {Project}},
	language = {en},
	author = {Dewald, William G and Thursby, Jerry G and Anderson, Richard G},
	year = {2020},
	keywords = {done},
	pages = {18},
}

@book{noauthor_access_2015-1,
	address = {LU},
	title = {Access to and {Preservation} of {Scientific} {Information} in {Europe}: {Report} on the {Implementation} of {Commission} {Recommendation} {C}(2012) 4890 {Final}.},
	shorttitle = {Access to and {Preservation} of {Scientific} {Information} in {Europe}},
	language = {eng},
	publisher = {Publications Office},
	year = {2015},
	keywords = {done},
}

@article{ezekiel_considerations_1933-1,
	title = {Some {Considerations} on the {Analysis} of the {Prices} of {Competing} or {Substitute} {Commodities}},
	volume = {1},
	issn = {00129682, 14680262},
	number = {2},
	journal = {Econometrica},
	author = {Ezekiel, Mordecai},
	year = {1933},
	note = {Publisher: [Wiley, Econometric Society]},
	keywords = {done},
	pages = {172--180},
}

@article{fomel_guest_2009-1,
	title = {Guest {Editors}' {Introduction}: {Reproducible} {Research}},
	volume = {11},
	number = {1},
	journal = {Computing in Science Engineering},
	author = {Fomel, S. and Claerbout, J. F.},
	year = {2009},
	keywords = {done},
	pages = {5--7},
}

@article{frisch_editors_1933-1,
	title = {Editor's {Note}},
	volume = {1},
	issn = {00129682, 14680262},
	number = {1},
	journal = {Econometrica},
	author = {Frisch, Ragnar},
	year = {1933},
	note = {Publisher: [Wiley, Econometric Society]},
	keywords = {done},
	pages = {1--4},
}

@article{gentleman_reproducible_2005-1,
	title = {Reproducible {Research}: {A} {Bioinformatics} {Case} {Study}},
	volume = {4},
	issn = {1544-6115, 2194-6302},
	shorttitle = {Reproducible {Research}},
	doi = {10.2202/1544-6115.1034},
	abstract = {While scientific research and the methodologies involved have gone through substantial technological evolution the technology involved in the publication of the results of these endeavors has remained relatively stagnant. Publication is largely done in the same manner today as it was fifty years ago. Many journals have adopted electronic formats, however, their orientation and style is little different from a printed document. The documents tend to be static and take little advantage of computational resources that might be available. Recent work, Gentleman and Temple Lang (2004), suggests a methodology and basic infrastructure that can be used to publish documents in a substantially different way. Their approach is suitable for the publication of papers whose message relies on computation. Stated quite simply, Gentleman and Temple Lang propose a paradigm where documents are mixtures of code and text. Such documents may be self-contained or they may be a component of a compendium which provides the infrastructure needed to provide access to data and supporting software. These documents, or compendiums, can be processed in a number of different ways. One transformation will be to replace the code with its output – thereby providing the familiar, but limited, static document.},
	language = {en},
	number = {1},
	journal = {Statistical Applications in Genetics and Molecular Biology},
	author = {Gentleman, Robert},
	month = jan,
	year = {2005},
	keywords = {done},
}

@article{gentleman_statistical_2007-1,
	title = {Statistical {Analyses} and {Reproducible} {Research}},
	volume = {16},
	doi = {10.1198/106186007X178663},
	abstract = {It is important, if not essential, to integrate the computations and code used in data analyses, methodological descriptions, simulations, and so on with the documents that describe and rely on them. This integration allows readers to both verify and adapt the claims in the documents. Authors can easily reproduce the results in the future, and they can present the document's contents in a different medium, for example, with interactive controls. This article describes a software framework for both authoring and distributing these integrated, dynamic documents that contain text, code, data, and any auxiliary content needed to recreate the computations. The documents are dynamic in that the contents— including figures, tables, and so on— can be recalculated each time a view of the document is generated. Our model treats a dynamic document as a master or “source” document from which one can generate different views in the form of traditional, derived documents for different audiences.We introduce the concept of a compendium as a container for one or more dynamic documents and the different elements needed when processing them, such as code and data. The compendium serves as a means for distributing, managing, and updating the collection.The step from disseminating analyses via a compendium to reproducible research is a small one. By reproducible research, we mean research papers with accompanying software tools that allow the reader to directly reproduce the results and employ the computational methods that are presented in the research paper. Some of the issues involved in paradigms for the production, distribution, and use of such reproducible research are discussed.},
	number = {1},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Gentleman, Robert and Lang, Duncan Temple},
	year = {2007},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/106186007X178663},
	keywords = {done},
	pages = {1--23},
}

@article{golub_molecular_1999-1,
	title = {Molecular {Classification} of {Cancer}: {Class} {Discovery} and {Class} {Prediction} by {Gene} {Expression} {Monitoring}},
	volume = {286},
	issn = {0036-8075},
	shorttitle = {Molecular {Classification} of {Cancer}},
	doi = {10.1126/science.286.5439.531},
	abstract = {Although cancer classification has improved over the past 30 years, there has been no general approach for identifying new cancer classes (class discovery) or for assigning tumors to known classes (class prediction). Here, a generic approach to cancer classification based on gene expression monitoring by DNA microarrays is described and applied to human acute leukemias as a test case. A class discovery procedure automatically discovered the distinction between acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL) without previous knowledge of these classes. An automatically derived class predictor was able to determine the class of new leukemia cases. The results demonstrate the feasibility of cancer classification based solely on gene expression monitoring and suggest a general strategy for discovering and predicting cancer classes for other types of cancer, independent of previous biological knowledge.},
	language = {eng},
	number = {5439},
	journal = {Science (New York, N.Y.)},
	author = {Golub, T. R. and Slonim, D. K. and Tamayo, P. and Huard, C. and Gaasenbeek, M. and Mesirov, J. P. and Coller, H. and Loh, M. L. and Downing, J. R. and Caligiuri, M. A. and Bloomfield, C. D. and Lander, E. S.},
	month = oct,
	year = {1999},
	pmid = {10521349},
	keywords = {done},
	pages = {531--537},
}

@article{goodman_what_2016-1,
	title = {What {Does} {Research} {Reproducibility} {Mean}?},
	volume = {8},
	issn = {1946-6234, 1946-6242},
	doi = {10.1126/scitranslmed.aaf5027},
	language = {en},
	number = {341},
	journal = {Science Translational Medicine},
	author = {Goodman, Steven N. and Fanelli, Daniele and Ioannidis, John P. A.},
	month = jun,
	year = {2016},
	keywords = {done},
	pages = {341ps12--341ps12},
}

@book{grolemund_r_nodate-1,
	title = {R for {Data} {Science}},
	abstract = {This book will teach you how to do data science with R: You'll learn how to get your data into R, get it into the most useful structure, transform it, visualise it and model it. In this book, you will find a practicum of skills for data science. Just as a chemist learns how to clean test tubes and stock a lab, you'll learn how to clean data and draw plots— and many other things besides. These are the skills that allow data science to happen, and here you will find the best practices for doing each of these things with R. You'll learn how to use the grammar of graphics, literate programming, and reproducible research to save time. You'll also learn how to manage cognitive resources to facilitate discoveries when wrangling, visualising, and exploring data.},
	author = {Grolemund, Garrett and Wickham, Hadley},
}

@misc{gruber_daring_nodate-2,
	title = {Daring {Fireball}: {Markdown}},
	url = {https://daringfireball.net/projects/markdown/},
	author = {Gruber, John and Swartz, Aron},
	keywords = {done},
}

@misc{gruber_daring_nodate-3,
	title = {Daring {Fireball}: {Markdown}},
	url = {https://daringfireball.net/projects/markdown/},
	author = {Gruber, John and Swartz, Aron},
	keywords = {done},
}

@article{ioannidis_why_2005-1,
	title = {Why {Most} {Published} {Research} {Findings} {Are} {False}},
	volume = {2},
	issn = {1549-1676},
	doi = {10.1371/journal.pmed.0020124},
	abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
	language = {en},
	number = {8},
	journal = {PLOS Medicine},
	author = {Ioannidis, John P. A.},
	month = aug,
	year = {2005},
	note = {Publisher: Public Library of Science},
	keywords = {done},
	pages = {e124},
}

@article{iyengar_selection_1988-1,
	title = {Selection {Models} and the {File} {Drawer} {Problem}},
	volume = {3},
	issn = {08834237},
	abstract = {Meta-analysis consists of quantitative methods for combining evidence from different studies about a particular issue. A frequent criticism of meta-analysis is that it may be based on a biased sample of all studies that were done. In this paper, we use selection models, or weighted distributions, to deal with one source of bias, namely, the failure to report studies that do not yield statistically significant results. We apply selection models to two approaches that have been suggested for correcting the bias. The fail-safe sample size approach calculates the minimum number of unpublished studies showing nonsignificant results that must have been carried out in order to overturn the conclusion reached from the published studies. The maximum likelihood approach uses a weighted distribution to model the selection bias in the generation of the data and estimates various parameters of interest. We suggest the use of families of weight functions to model plausible biasing mechanisms to study the sensitivity of inferences about effect sizes. By using an example, we show that the maximum likelihood approach has several advantages over the fail-safe sample size approach.},
	number = {1},
	journal = {Statistical Science},
	author = {Iyengar, Satish and Greenhouse, Joel B.},
	year = {1988},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {done},
	pages = {109--117},
}

@article{jasny_again_2011-1,
	title = {Again, and {Again}, and {Again}},
	volume = {334},
	issn = {0036-8075},
	doi = {10.1126/science.334.6060.1225},
	number = {6060},
	journal = {Science},
	author = {Jasny, Barbara R. and Chin, Gilbert and Chong, Lisa and Vignieri, Sacha},
	year = {2011},
	note = {Publisher: American Association for the Advancement of Science
\_eprint: https://science.sciencemag.org/content/334/6060/1225.full.pdf},
	keywords = {done},
	pages = {1225--1225},
}

@article{klein_many_2018-1,
	title = {Many {Labs} 2: {Investigating} {Variation} in {Replicability} across {Samples} and {Settings}},
	volume = {1},
	doi = {10.1177/2515245918810225},
	abstract = {We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p ¡ .05), we found that 15 (54\%) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p ¡ .0001), 14 (50\%) of the replications still provided such evidence, a reflection of the extremely high-powered design. Seven (25\%) of the replications yielded effect sizes larger than the original ones, and 21 (75\%) yielded effect sizes smaller than the original ones. The median comparable Cohen's ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small (¡ 0.20) in 16 of the replications (57\%), and 9 effects (32\%) were in the direction opposite the direction of the original effect. Across settings, the Q statistic indicated significant heterogeneity in 11 (39\%) of the replication effects, and most of those were among the findings with the largest overall effect sizes; only 1 effect that was near zero in the aggregate showed significant heterogeneity according to this measure. Only 1 effect had a tau value greater than .20, an indication of moderate heterogeneity. Eight others had tau values near or slightly above .10, an indication of slight heterogeneity. Moderation tests indicated that very little heterogeneity was attributable to the order in which the tasks were performed or whether the tasks were administered in lab versus online. Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized, rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness scores, respectively). Cumulatively, variability in the observed effect sizes was attributable more to the effect being studied than to the sample or setting in which it was studied.},
	number = {4},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Klein, Richard A. and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G. and Reginald B. Adams, Jr. and Alper, Sinan and Aveyard, Mark and Axt, Jordan R. and Babalola, Mayowa T. and Bahník, Štěpán and Batra, Rishtee and Berkics, Mihály and Bernstein, Michael J. and Berry, Daniel R. and Bialobrzeska, Olga and Binan, Evans Dami and Bocian, Konrad and Brandt, Mark J. and Busching, Robert and Rédei, Anna Cabak and Cai, Huajian and Cambier, Fanny and Cantarero, Katarzyna and Carmichael, Cheryl L. and Ceric, Francisco and Chandler, Jesse and Chang, Jen-Ho and Chatard, Armand and Chen, Eva E. and Cheong, Winnee and Cicero, David C. and Coen, Sharon and Coleman, Jennifer A. and Collisson, Brian and Conway, Morgan A. and Corker, Katherine S. and Curran, Paul G. and Cushman, Fiery and Dagona, Zubairu K. and Dalgar, Ilker and Rosa, Anna Dalla and Davis, William E. and de Bruijn, Maaike and Schutter, Leander De and Devos, Thierry and de Vries, Marieke and Doğulu, Canay and Dozo, Nerisa and Dukes, Kristin Nicole and Dunham, Yarrow and Durrheim, Kevin and Ebersole, Charles R. and Edlund, John E. and Eller, Anja and English, Alexander Scott and Finck, Carolyn and Frankowska, Natalia and Freyre, Miguel-Ángel and Friedman, Mike and Galliani, Elisa Maria and Gandi, Joshua C. and Ghoshal, Tanuka and Giessner, Steffen R. and Gill, Tripat and Gnambs, Timo and Gómez, Ángel and González, Roberto and Graham, Jesse and Grahe, Jon E. and Grahek, Ivan and Green, Eva G. T. and Hai, Kakul and Haigh, Matthew and Haines, Elizabeth L. and Hall, Michael P. and Heffernan, Marie E. and Hicks, Joshua A. and Houdek, Petr and Huntsinger, Jeffrey R. and Huynh, Ho Phi and IJzerman, Hans and Inbar, Yoel and Innes-Ker, Åse H. and Jiménez-Leal, William and John, Melissa-Sue and Joy-Gaba, Jennifer A. and Kamiloğlu, Roza G. and Kappes, Heather Barry and Karabati, Serdar and Karick, Haruna and Keller, Victor N. and Kende, Anna and Kervyn, Nicolas and Knežević, Goran and Kovacs, Carrie and Krueger, Lacy E. and Kurapov, German and Kurtz, Jamie and Lakens, Daniël and Lazarević, Ljiljana B. and Levitan, Carmel A. and Neil A. Lewis, Jr. and Lins, Samuel and Lipsey, Nikolette P. and Losee, Joy E. and Maassen, Esther and Maitner, Angela T. and Malingumu, Winfrida and Mallett, Robyn K. and Marotta, Satia A. and Me{\textbackslash}djedović, Janko and Mena-Pacheco, Fernando and Milfont, Taciano L. and Morris, Wendy L. and Murphy, Sean C. and Myachykov, Andriy and Neave, Nick and Neijenhuijs, Koen and Nelson, Anthony J. and Neto, Félix and Nichols, Austin Lee and Ocampo, Aaron and O'Donnell, Susan L. and Oikawa, Haruka and Oikawa, Masanori and Ong, Elsie and Orosz, Gábor and Osowiecka, Malgorzata and Packard, Grant and Pérez-Sánchez, Rolando and Petrović, Boban and Pilati, Ronaldo and Pinter, Brad and Podesta, Lysandra and Pogge, Gabrielle and Pollmann, Monique M. H. and Rutchick, Abraham M. and Saavedra, Patricio and Saeri, Alexander K. and Salomon, Erika and Schmidt, Kathleen and Schönbrodt, Felix D. and Sekerdej, Maciej B. and Sirlopú, David and Skorinko, Jeanine L. M. and Smith, Michael A. and Smith-Castro, Vanessa and Smolders, Karin C. H. J. and Sobkow, Agata and Sowden, Walter and Spachtholz, Philipp and Srivastava, Manini and Steiner, Troy G. and Stouten, Jeroen and Street, Chris N. H. and Sundfelt, Oskar K. and Szeto, Stephanie and Szumowska, Ewa and Tang, Andrew C. W. and Tanzer, Norbert and Tear, Morgan J. and Theriault, Jordan and Thomae, Manuela and Torres, David and Traczyk, Jakub and Tybur, Joshua M. and Ujhelyi, Adrienn and van Aert, Robbie C. M. and van Assen, Marcel A. L. M. and van der Hulst, Marije and van Lange, Paul A. M. and van 't Veer, Anna Elisabeth and Echeverría, Alejandro Vásquez- and Vaughn, Leigh Ann and Vázquez, Alexandra and Vega, Luis Diego and Verniers, Catherine and Verschoor, Mark and Voermans, Ingrid P. J. and Vranka, Marek A. and Welch, Cheryl and Wichman, Aaron L. and Williams, Lisa A. and Wood, Michael and Woodzicka, Julie A. and Wronska, Marta K. and Young, Liane and Zelenski, John M. and Zhijia, Zeng and Nosek, Brian A.},
	year = {2018},
	note = {\_eprint: https://doi.org/10.1177/2515245918810225},
	keywords = {done},
	pages = {443--490},
}

@article{knuth_literate_1984-1,
	title = {Literate {Programming}},
	volume = {27},
	issn = {0010-4620},
	doi = {10.1093/comjnl/27.2.97},
	abstract = {The author and his associates have been experimenting for the past several years with a programming language and documentation system called WEB. This paper presents WEB by example, and discusses why the new system appears to be an improvement over previous ones.},
	number = {2},
	journal = {The Computer Journal},
	author = {Knuth, D. E.},
	month = jan,
	year = {1984},
	note = {\_eprint: https://academic.oup.com/comjnl/article-pdf/27/2/97/981657/270097.pdf},
	keywords = {done},
	pages = {97--111},
}

@book{knuth_texbook_1986-1,
	series = {Computers \& {Typesetting}},
	title = {The {TeXbook}},
	isbn = {978-0-201-13447-6},
	publisher = {Addison-Wesley},
	author = {Knuth, D.E.},
	year = {1986},
	lccn = {85030845},
	keywords = {done},
}

@book{knuth_literate_1992-1,
	title = {Literate {Programming}},
	isbn = {978-0-937073-81-0},
	abstract = {Literate programming is a programming methodology that combines a programming language with a documentation language, making programs more robust, more portable, and more easily maintained than programs written only in a high-level language. Computer programmers already know both kinds of languages; they need only learn a few conventions about alternating between languages to create programs that are works of literature. A literate programmer is an essayist who writes programs for humans to understand, instead of primarily writing instructions for machines to follow. When programs are written in the recommended style they can be transformed into documents by a document compiler and into efficient code by an algebraic compiler. This anthology of essays from the inventor of literate programming includes Knuth's early papers on related topics such as structured programming, as well as the Computer Journal article that launched literate programming itself.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Knuth, Donald E.},
	month = mar,
	year = {1992},
	keywords = {done},
}

@book{lander_r_2017-1,
	address = {Boston},
	edition = {2nd Edition},
	title = {R for {Everyone}: {Advanced} {Analytics} and {Graphics}},
	isbn = {978-0-13-454692-6},
	shorttitle = {R for {Everyone}},
	abstract = {Statistical Computation for Programmers, Scientists, Quants, Excel Users, and Other Professionals Using the open source R language, you can build powerful statistical models to answer many of your most challenging questions. R has traditionally been difficult for non-statisticians to learn, and most R books assume far too much knowledge to be of help. R for Everyone, Second Edition, is the solution. Drawing on his unsurpassed experience teaching new users, professional data scientist Jared P. Lander has written the perfect tutorial for anyone new to statistical programming and modeling. Organized to make learning easy and intuitive, this guide focuses on the 20 percent of R functionality you'll need to accomplish 80 percent of modern data tasks. Lander's self-contained chapters start with the absolute basics, offering extensive hands-on practice and sample code. You'll download and install R; navigate and use the R environment; master basic program control, data import, manipulation, and visualization; and walk through several essential tests. Then, building on this foundation, you'll construct several complete models, both linear and nonlinear, and use some data mining techniques. After all this you'll make your code reproducible with LaTeX, RMarkdown, and Shiny. By the time you're done, you won't just know how to write R programs, you'll be ready to tackle the statistical problems you care about most. Coverage includes Explore R, RStudio, and R packages Use R for math: variable types, vectors, calling functions, and more Exploit data structures, including data.frames, matrices, and lists Read many different types of data Create attractive, intuitive statistical graphics Write user-defined functions Control program flow with if, ifelse, and complex checks Improve program efficiency with group manipulations Combine and reshape multiple datasets Manipulate strings using R's facilities and regular expressions Create normal, binomial, and Poisson probability distributions Build linear, generalized linear, and nonlinear models Program basic statistics: mean, standard deviation, and t-tests Train machine learning models Assess the quality of models and variable selection Prevent overfitting and perform variable selection, using the Elastic Net and Bayesian methods Analyze univariate and multivariate time series data Group data via K-means and hierarchical clustering Prepare reports, slideshows, and web pages with knitr Display interactive data with RMarkdown and htmlwidgets Implement dashboards with Shiny Build reusable R packages with devtools and Rcpp Register your product at informit.com/register for convenient access to downloads, updates, and corrections as they become available.},
	language = {English},
	publisher = {Addison-Wesley Professional},
	author = {Lander, Jared P.},
	month = jun,
	year = {2017},
}

@incollection{leisch_sweave_2002-1,
	address = {Heidelberg},
	title = {Sweave: {Dynamic} {Generation} of {Statistical} {Reports} {Using} {Literate} {Data} {Analysis}},
	isbn = {978-3-7908-1517-7 978-3-642-57489-4},
	shorttitle = {Sweave},
	abstract = {Sweave combines typesetting with LATEX and data anlysis with S into integrated statistical documents. When run through R or Splus, all data analysis output (tables, graphs, . . . ) is created on the fly and inserted into a final LATEX document. Options control which parts of the original S code are shown to or hidden from the reader, respectively. Many S users are also LATEX users, hence no new software has to be learned. The report can be automatically updated if data or analysis change, which allows for truly reproducible research.},
	language = {en},
	booktitle = {Compstat},
	publisher = {Physica-Verlag HD},
	author = {Leisch, Friedrich},
	editor = {Härdle, Wolfgang and Rönz, Bernd},
	year = {2002},
	doi = {10.1007/978-3-642-57489-4_89},
	keywords = {done},
	pages = {575--580},
}

@article{markowetz_five_2015-1,
	title = {Five {Selfish} {Reasons} to {Work} {Reproducibly}},
	volume = {16},
	issn = {1474-760X},
	doi = {10.1186/s13059-015-0850-7},
	language = {en},
	number = {1},
	journal = {Genome Biology},
	author = {Markowetz, Florian},
	month = dec,
	year = {2015},
	pages = {274},
}

@article{mccullough_economics_2008-1,
	title = {Do {Economics} {Journal} {Archives} {Promote} {Replicable} {Research}?},
	volume = {41},
	copyright = {© Canadian Economics Association},
	issn = {1540-5982},
	doi = {10.1111/j.1540-5982.2008.00509.x},
	abstract = {Abstract. All the long-standing archives at economics journals do not facilitate the reproduction of published results. The data-only archives at Journal of Business and Economic Statistics and Economic Journal fail in part because most authors do not contribute data. Results published in the FRB St. Louis Review can rarely be reproduced using the data+code in the journal archive. Recently created archives at top journals should avoid the mistakes of their predecessors. We categorize reasons for archives' failures and identify successful policies.},
	language = {en},
	number = {4},
	journal = {Canadian Journal of Economics/Revue canadienne d'économique},
	author = {McCullough, B. D. and McGeary, Kerry Anne and Harrison, Teresa D.},
	year = {2008},
	keywords = {done},
	pages = {1406--1420},
}

@article{mcnutt_reproducibility_2014-1,
	title = {Reproducibility},
	volume = {343},
	issn = {0036-8075},
	doi = {10.1126/science.1250475},
	number = {6168},
	journal = {Science},
	author = {McNutt, Marcia},
	year = {2014},
	note = {Publisher: American Association for the Advancement of Science
\_eprint: https://science.sciencemag.org/content/343/6168/229.full.pdf},
	keywords = {done},
	pages = {229--229},
}

@article{nosek_promoting_2015-1,
	title = {Promoting an {Open} {Research} {Culture}},
	volume = {348},
	issn = {0036-8075, 1095-9203},
	doi = {10.1126/science.aab2374},
	language = {en},
	number = {6242},
	journal = {Science},
	author = {Nosek, B. A. and Alter, G. and Banks, G. C. and Borsboom, D. and Bowman, S. D. and Breckler, S. J. and Buck, S. and Chambers, C. D. and Chin, G. and Christensen, G. and Contestabile, M. and Dafoe, A. and Eich, E. and Freese, J. and Glennerster, R. and Goroff, D. and Green, D. P. and Hesse, B. and Humphreys, M. and Ishiyama, J. and Karlan, D. and Kraut, A. and Lupia, A. and Mabry, P. and Madon, T. and Malhotra, N. and Mayo-Wilson, E. and McNutt, M. and Miguel, E. and Paluck, E. L. and Simonsohn, U. and Soderberg, C. and Spellman, B. A. and Turitto, J. and VandenBos, G. and Vazire, S. and Wagenmakers, E. J. and Wilson, R. and Yarkoni, T.},
	month = jun,
	year = {2015},
	keywords = {done},
	pages = {1422--1425},
}

@article{nust_opening_2017-1,
	title = {Opening the {Publication} {Process} with {Executable} {Research} {Compendia}},
	volume = {23},
	issn = {1082-9873},
	doi = {10.1045/january2017-nuest},
	language = {en},
	number = {1/2},
	journal = {D-Lib Magazine},
	author = {Nüst, Daniel and Konkol, Markus and Pebesma, Edzer and Kray, Christian and Schutzeichel, Marc and Przibytzin, Holger and Lorenz, Jörg},
	month = jan,
	year = {2017},
}

@article{collaboration_estimating_2015-1,
	title = {Estimating the {Reproducibility} of {Psychological} {Science}},
	volume = {349},
	issn = {0036-8075},
	doi = {10.1126/science.aac4716},
	abstract = {One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.Science, this issue 10.1126/science.aac4716INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P \&lt; .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that we already know this belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
	number = {6251},
	journal = {Science},
	author = {Collaboration, Open Science},
	year = {2015},
	note = {Publisher: American Association for the Advancement of Science
\_eprint: https://science.sciencemag.org/content/349/6251/aac4716.full.pdf},
	keywords = {done},
}

@article{peng_reproducible_2011-1,
	title = {Reproducible {Research} in {Computational} {Science}},
	volume = {334},
	copyright = {Copyright © 2011, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	doi = {10.1126/science.1213847},
	abstract = {Computational science has led to exciting new developments, but the nature of the work has exposed limitations in our ability to evaluate published findings. Reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible.},
	language = {en},
	number = {6060},
	journal = {Science},
	author = {Peng, Roger D.},
	month = dec,
	year = {2011},
	pmid = {22144613},
	note = {Publisher: American Association for the Advancement of Science
Section: Perspective},
	keywords = {done},
	pages = {1226--1227},
}

@article{ram_git_2013-1,
	title = {Git {Can} {Facilitate} {Greater} {Reproducibility} and {Increased} {Transparency} in {Science}},
	volume = {8},
	issn = {1751-0473},
	doi = {10.1186/1751-0473-8-7},
	abstract = {Reproducibility is the hallmark of good science. Maintaining a high degree of transparency in scientific reporting is essential not just for gaining trust and credibility within the scientific community but also for facilitating the development of new ideas. Sharing data and computer code associated with publications is becoming increasingly common, motivated partly in response to data deposition requirements from journals and mandates from funders. Despite this increase in transparency, it is still difficult to reproduce or build upon the findings of most scientific publications without access to a more complete workflow.},
	number = {1},
	journal = {Source Code for Biology and Medicine},
	author = {Ram, Karthik},
	month = feb,
	year = {2013},
	pages = {7},
}

@misc{ramsey_noweb_nodate-1,
	title = {Noweb {Home} {Page}},
	url = {https://www.cs.tufts.edu/\textasciitilde nr/noweb/},
	author = {Ramsey, Norman},
	keywords = {done},
}

@book{r_core_team_r_2020-1,
	address = {Vienna, Austria},
	title = {R: {A} {Language} and {Environment} for {Statistical} {Computing}},
	publisher = {R Foundation for Statistical Computing},
	author = {{R Core Team}},
	year = {2020},
	keywords = {done},
}

@book{riederer_r_nodate-1,
	title = {R {Markdown} {Cookbook}},
	abstract = {This book showcases short, practical examples of lesser-known tips and tricks to helps users get the most out of these tools. After reading this book, you will understand how R Markdown documents are transformed from plain text and how you may customize nearly every step of this processing. For example, you will learn how to dynamically create content from R code, reference code in other documents or chunks, control the formatting with customer templates, fine-tune how your code is processed, and incorporate multiple languages into your analysis.},
	author = {Riederer, Christophe Dervieux, Emily, Yihui Xie},
	keywords = {done},
}

@inproceedings{rosenthal_file_1979-1,
	title = {The {File} {Drawer} {Problem} and {Tolerance} for {Null} {Results}.},
	volume = {86},
	publisher = {Psychological Bulletin},
	author = {Rosenthal, R.},
	year = {1979},
	keywords = {done},
	pages = {638--641},
}

@book{rstudio_team_rstudio_2020-1,
	address = {Boston, MA},
	title = {{RStudio}: {Integrated} {Development} {Environment} for r},
	publisher = {RStudio, PBC.},
	author = {{RStudio Team}},
	year = {2020},
}

@article{schwab_reproducible_1995-1,
	title = {Reproducible {Electronic} {Documents}},
	abstract = {To organize computational scientific research and hence to conveniently transfer our technology, we impose a simple filing discipline on the authors in our laboratory. A document's makefile includes laboratory-wide standard rules that offer readers these four standard commands: make burn removes the document's result figures, make build recomputes them, make view displays the figures, and make clean removes any intermediate files. Although we developed these standards to aid readers we discovered that authors are often the principal beneficiaries.},
	language = {en},
	author = {Schwab, Matthias and Karrenbach, Martin and Claerbout, Jon},
	year = {1995},
	keywords = {done},
	pages = {14},
}

@article{simmons_false-positive_2011-1,
	title = {False-{Positive} {Psychology}: {Undisclosed} {Flexibility} in {Data} {Collection} and {Analysis} {Allows} {Presenting} {Anything} as {Significant}},
	volume = {22},
	doi = {10.1177/0956797611417632},
	abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings (\${\textbackslash}leq\$ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
	number = {11},
	journal = {Psychological Science},
	author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
	year = {2011},
	pmid = {22006061},
	note = {\_eprint: https://doi.org/10.1177/0956797611417632},
	keywords = {done},
	pages = {1359--1366},
}

@book{tierney_rmarkdown_nodate-1,
	title = {{RMarkdown} for {Scientists}},
	abstract = {A book created for a 3 hour workshop on rmarkdown},
	author = {Tierney, Nicholas},
	keywords = {done},
}

@book{wickham_r_2016-1,
	address = {Beijing},
	title = {R for {Data} {Science}: {Import}, {Tidy}, {Transform}, {Visualize}, and {Model} {Data}},
	isbn = {978-1-4919-1039-9},
	shorttitle = {R for {Data} {Science}},
	language = {eng},
	publisher = {O'Reilly},
	author = {Wickham, Hadley and Grolemund, Garrett},
	year = {2016},
	lccn = {006.312 Wic, 519.2 W63r, 006.312 WIC},
}

@article{wikipedia_meta-analysis_2020-1,
	title = {Meta-{Analysis}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	abstract = {A meta-analysis is a statistical analysis that combines the results of multiple scientific studies. Meta-analysis can be performed when there are multiple scientific studies addressing the same question, with each individual study reporting measurements that are expected to have some degree of error. The aim then is to use approaches from statistics to derive a pooled estimate closest to the unknown common truth based on how this error is perceived. Existing methods for meta-analysis yield a weighted average from the results of the individual studies, and what differs is the manner in which these weights are allocated and also the manner in which the uncertainty is computed around the point estimate thus generated. In addition to providing an estimate of the unknown common truth, meta-analysis has the capacity to contrast results from different studies and identify patterns among study results, sources of disagreement among those results, or other interesting relationships that may come to light in the context of multiple studies.A key benefit of this approach is the aggregation of information leading to a higher statistical power and more robust point estimate than is possible from the measure derived from any individual study. However, in performing a meta-analysis, an investigator must make choices which can affect the results, including deciding how to search for studies, selecting studies based on a set of objective criteria, dealing with incomplete data, analyzing the data, and accounting for or choosing not to account for publication bias. Judgment calls made in completing a meta-analysis may affect the results. For example, Wanous and colleagues examined four pairs of meta-analyses on the four topics of (a) job performance and satisfaction relationship, (b) realistic job previews, (c) correlates of role conflict and ambiguity, and (d) the job satisfaction and absenteeism relationship, and illustrated how various judgement calls made by the researchers produced different results.Meta-analyses are often, but not always, important components of a systematic review procedure. For instance, a meta-analysis may be conducted on several clinical trials of a medical treatment, in an effort to obtain a better understanding of how well the treatment works. Here it is convenient to follow the terminology used by the Cochrane Collaboration, and use "meta-analysis" to refer to statistical methods of combining evidence, leaving other aspects of 'research synthesis' or 'evidence synthesis', such as combining information from qualitative studies, for the more general context of systematic reviews. A meta-analysis is a secondary source.},
	language = {en},
	journal = {Wikipedia},
	author = {{wikipedia}},
	month = aug,
	year = {2020},
	keywords = {done},
}

@incollection{xie_knitr_2014-1,
	title = {Knitr: {A} {Comprehensive} {Tool} for {Reproducible} {Research} in {R}},
	booktitle = {Implementing {Reproducible} {Computational} {Research}},
	publisher = {Chapman and Hall/CRC},
	author = {Xie, Yihui},
	editor = {Stodden, Victoria and Leisch, Friedrich and Peng, Roger D.},
	year = {2014},
	keywords = {done},
	annote = {ISBN 978-1466561595},
}

@book{xie_dynamic_2015-1,
	address = {Boca Raton, Florida},
	edition = {Second},
	title = {Dynamic {Documents} with {R} and {Knitr}},
	publisher = {Chapman and Hall/CRC},
	author = {Xie, Yihui},
	year = {2015},
	keywords = {done},
	annote = {ISBN 978-1498716963},
}

@book{xie_r_2018-1,
	address = {Boca Raton, Florida},
	title = {R {Markdown}: {The} {Definitive} {Guide}},
	publisher = {Chapman and Hall/CRC},
	author = {Xie, Yihui and Allaire, J.J. and Grolemund, Garrett},
	year = {2018},
	keywords = {done},
	annote = {ISBN 9781138359338},
}

@book{xie_knitr_2020-1,
	title = {Knitr: {A} {General}-{Purpose} {Package} for {Dynamic} {Report} {Generation} in r},
	author = {Xie, Yihui},
	year = {2020},
	note = {Type: Manual},
	keywords = {done},
	annote = {R package version 1.29},
}

@article{young_why_2008-1,
	title = {Why {Current} {Publication} {Practices} {May} {Distort} {Science}},
	volume = {5},
	issn = {1549-1676},
	doi = {10.1371/journal.pmed.0050201},
	abstract = {The current system of publication in biomedical research provides a distorted view of the reality of scientific data that are generated in the laboratory and clinic. This system can be studied by applying principles from the field of economics. The “winner's curse,” a more general statement of publication bias, suggests that the small proportion of results chosen for publication are unrepresentative of scientists' repeated samplings of the real world. The self-correcting mechanism in science is retarded by the extreme imbalance between the abundance of supply (the output of basic science laboratories and clinical investigations) and the increasingly limited venues for publication (journals with sufficiently high impact). This system would be expected intrinsically to lead to the misallocation of resources. The scarcity of available outlets is artificial, based on the costs of printing in an electronic age and a belief that selectivity is equivalent to quality. Science is subject to great uncertainty: we cannot be confident now which efforts will ultimately yield worthwhile achievements. However, the current system abdicates to a small number of intermediates an authoritative prescience to anticipate a highly unpredictable future. In considering society's expectations and our own goals as scientists, we believe that there is a moral imperative to reconsider how scientific data are judged and disseminated.},
	language = {en},
	number = {10},
	journal = {PLoS Medicine},
	author = {Young, Neal S and Ioannidis, John P. A and Al-Ubaydli, Omar},
	month = oct,
	year = {2008},
	keywords = {done},
	pages = {e201},
}

@misc{noauthor_dear_nodate-2,
	title = {Dear {Colleague} {Letter}: {Robust} and {Reliable} {Research} in the {Social}, {Behavioral}, and {Economic} {Sciences} ({Nsf16137}) {\textbar} {NSF} - {National} {Science} {Foundation}},
	url = {https://www.nsf.gov/pubs/2016/nsf16137/nsf16137.jsp},
	keywords = {done},
}

@misc{noauthor_dear_nodate-3,
	title = {Dear {Colleague} {Letter}: {Robust} and {Reliable} {Research} in the {Social}, {Behavioral}, and {Economic} {Sciences} ({Nsf16137}) {\textbar} {NSF} - {National} {Science} {Foundation}},
	url = {https://www.nsf.gov/pubs/2016/nsf16137/nsf16137.jsp},
	keywords = {done},
}

@misc{noauthor_american_nodate-1,
	title = {American {Economic} {Association}},
	url = {https://www.aeaweb.org/journals/data},
}

@misc{noauthor_pandoc_nodate-1,
	title = {Pandoc - {Pandoc} {User}'s {Guide}},
	url = {https://pandoc.org/MANUAL.html},
}

@article{nosek_estimating_2015-1,
	title = {Estimating the {Reproducibility} of {Psychological} {Science}},
	volume = {349},
	issn = {0036-8075},
	doi = {10.1126/science.aac4716},
	abstract = {One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.Science, this issue 10.1126/science.aac4716INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P \&lt; .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that we already know this belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
	number = {6251},
	journal = {Science},
	author = {Nosek, Brian A. and {et al}},
	year = {2015},
	note = {Publisher: American Association for the Advancement of Science
\_eprint: https://science.sciencemag.org/content/349/6251/aac4716.full.pdf},
	keywords = {done},
}

@misc{saetrevik_replikasjonskrisen_2017,
	title = {Replikasjonskrisen},
	url = {https://psykologtidsskriftet.no/fagessay/2017/07/replikasjonskrisen},
	language = {Norsk},
	author = {Sætrevik, B.},
	year = {2017},
}

@article{leek_opinion_nodate,
	chapter = {6},
	edition = {112},
	title = {Opinion: {Reproducible} research can still be wrong: {Adopting} a prevention approach.},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Leek, J. T and Peng, R. D.},
	pages = {1645--1646},
}

@article{noauthor_notitle_nodate,
}

@article{schmidt_reproducible_2015,
	title = {Reproducible {Research} {Using} {RMarkdown} and {Git} through {Rstudio}},
	url = {https://rpubs.com/marschmi/105639>},
	journal = {RPubs by Rstudio},
	author = {Schmidt, M. L.},
	year = {2015},
}

@book{allaire_rmarkdown_2020-2,
	title = {Rmarkdown: {Dynamic} {Documents} for r},
	author = {Allaire, JJ and Xie, Yihui and McPherson, Jonathan and Luraschi, Javier and Ushey, Kevin and Atkins, Aron and Wickham, Hadley and Cheng, Joe and Chang, Winston and Iannone, Richard},
	year = {2020},
	keywords = {done},
	annote = {R package version 2.2},
}

@article{barnes_publish_2010-2,
	title = {Publish {Your} {Computer} {Code}: {It} {Is} {Good} {Enough}},
	volume = {467},
	issn = {0028-0836, 1476-4687},
	shorttitle = {Publish {Your} {Computer} {Code}},
	doi = {10.1038/467753a},
	language = {en},
	number = {7317},
	journal = {Nature},
	author = {Barnes, Nick},
	month = oct,
	year = {2010},
	pages = {753--753},
}

@article{bechhofer_why_2013-2,
	title = {Why {Linked} {Data} {Is} {Not} {Enough} for {Scientists}},
	volume = {29},
	issn = {0167739X},
	doi = {10.1016/j.future.2011.08.004},
	language = {en},
	number = {2},
	journal = {Future Generation Computer Systems},
	author = {Bechhofer, Sean and Buchan, Iain and De Roure, David and Missier, Paolo and Ainsworth, John and Bhagat, Jiten and Couch, Philip and Cruickshank, Don and Delderfield, Mark and Dunlop, Ian and Gamble, Matthew and Michaelides, Danius and Owen, Stuart and Newman, David and Sufi, Shoaib and Goble, Carole},
	month = feb,
	year = {2013},
	keywords = {done},
	pages = {599--611},
}

@techreport{bollen_social_2015-2,
	title = {Social, {Behavioral}, and {Economic} {Sciences} {Perspectives} on {Robust} and {Reliable} {Science}},
	number = {Report of the Subcommittee on Replicability in Science Advisory Committee to the National Science Foundation Directorate for Social, Behavioral, and Economic Sciences},
	institution = {NSF},
	author = {Bollen, Kenneth and Cacioppo, John T. and Krosnick, Jon A. and Olds, James L. and Kaplan, Robert M.},
	year = {2015},
	keywords = {done},
}

@inproceedings{brase_datacite_2009-2,
	address = {Beijing, China},
	title = {{DataCite} - {A} {Global} {Registration} {Agency} for {Research} {Data}},
	isbn = {978-0-7695-3898-3},
	doi = {10.1109/COINFO.2009.66},
	booktitle = {2009 {Fourth} {International} {Conference} on {Cooperation} and {Promotion} of {Information} {Resources} in {Science} and {Technology}},
	publisher = {IEEE},
	author = {Brase, Jan},
	month = nov,
	year = {2009},
	keywords = {done},
	pages = {257--261},
}

@incollection{buckheit_wavelab_1995-2,
	address = {New York, NY},
	title = {{WaveLab} and {Reproducible} {Research}},
	volume = {103},
	isbn = {978-0-387-94564-4 978-1-4612-2544-7},
	abstract = {WaveLab is a library of Matlab routines for wavelet analysis, wavelet-packet analysis, cosine-packet analysis and matching pursuit. The library is available free of charge over the Internet. Versions are provided for Macintosh, UNIX and Windows machines.},
	language = {en},
	booktitle = {Wavelets and {Statistics}},
	publisher = {Springer New York},
	author = {Buckheit, Jonathan B. and Donoho, David L.},
	editor = {Bickel, P. and Diggle, P. and Fienberg, S. and Krickeberg, K. and Olkin, I. and Wermuth, N. and Zeger, S. and Antoniadis, Anestis and Oppenheim, Georges},
	year = {1995},
	doi = {10.1007/978-1-4612-2544-7_5},
	keywords = {done},
	pages = {55--81},
}

@misc{conyers_learn_nodate-2,
	title = {Learn {Git} in 3 {Hours}},
	url = {https://www.safaribooksonline.com/videos/learn-git-in/9781789348231/9781789348231-video1_2},
	abstract = {Build powerful and effective projects using Git Version Control Systems About This Video Learn how to create, contribute to, and collaborate on software projects using Git Understand its fundamental features,...},
	author = {Conyers, Ross},
	annote = {Publication Title: O'Reilly {\textbackslash}textbar Safari},
}

@article{dewald_replication_1986-2,
	title = {Replication in {Empirical} {Economics}: {The} {Journal} of {Money}, {Credit} and {Banking} {Project}},
	volume = {76},
	issn = {0002-8282},
	shorttitle = {Replication in {Empirical} {Economics}},
	abstract = {This paper examines the role of replication in empirical economic research. It presents the findings of a two-year study that collected programs and data from authors and attempted to replicate their published results. Our research provides new and important information about the extent and causes of failures to replicate published results in economics. Our findings suggest that inadvertent errors in published empirical articles are a commonplace rather thana rare occurrence.},
	number = {4},
	journal = {The American Economic Review},
	author = {Dewald, William G. and Thursby, Jerry G. and Anderson, Richard G.},
	year = {1986},
	keywords = {done},
	pages = {587--603},
	annote = {Publisher: American Economic Association},
}

@article{dewald_replication_2020-2,
	title = {Replication in {Empirical} {Economics}: {The} {Journal} of {Money}, {Credit} and {Banking} {Project}},
	language = {en},
	author = {Dewald, William G and Thursby, Jerry G and Anderson, Richard G},
	year = {2020},
	keywords = {done},
	pages = {18},
}

@book{noauthor_access_2015-2,
	address = {LU},
	title = {Access to and {Preservation} of {Scientific} {Information} in {Europe}: {Report} on the {Implementation} of {Commission} {Recommendation} {C}(2012) 4890 {Final}.},
	shorttitle = {Access to and {Preservation} of {Scientific} {Information} in {Europe}},
	language = {eng},
	publisher = {Publications Office},
	year = {2015},
	keywords = {done},
}

@article{ezekiel_considerations_1933-2,
	title = {Some {Considerations} on the {Analysis} of the {Prices} of {Competing} or {Substitute} {Commodities}},
	volume = {1},
	issn = {00129682, 14680262},
	number = {2},
	journal = {Econometrica},
	author = {Ezekiel, Mordecai},
	year = {1933},
	keywords = {done},
	pages = {172--180},
	annote = {Publisher: [Wiley, Econometric Society]},
}

@article{fomel_guest_2009-2,
	title = {Guest {Editors}' {Introduction}: {Reproducible} {Research}},
	volume = {11},
	number = {1},
	journal = {Computing in Science Engineering},
	author = {Fomel, S. and Claerbout, J. F.},
	year = {2009},
	keywords = {done},
	pages = {5--7},
}

@article{frisch_editors_1933-2,
	title = {Editor's {Note}},
	volume = {1},
	issn = {00129682, 14680262},
	number = {1},
	journal = {Econometrica},
	author = {Frisch, Ragnar},
	year = {1933},
	keywords = {done},
	pages = {1--4},
	annote = {Publisher: [Wiley, Econometric Society]},
}

@article{gentleman_reproducible_2005-2,
	title = {Reproducible {Research}: {A} {Bioinformatics} {Case} {Study}},
	volume = {4},
	issn = {1544-6115, 2194-6302},
	shorttitle = {Reproducible {Research}},
	doi = {10.2202/1544-6115.1034},
	abstract = {While scientific research and the methodologies involved have gone through substantial technological evolution the technology involved in the publication of the results of these endeavors has remained relatively stagnant. Publication is largely done in the same manner today as it was fifty years ago. Many journals have adopted electronic formats, however, their orientation and style is little different from a printed document. The documents tend to be static and take little advantage of computational resources that might be available. Recent work, Gentleman and Temple Lang (2004), suggests a methodology and basic infrastructure that can be used to publish documents in a substantially different way. Their approach is suitable for the publication of papers whose message relies on computation. Stated quite simply, Gentleman and Temple Lang propose a paradigm where documents are mixtures of code and text. Such documents may be self-contained or they may be a component of a compendium which provides the infrastructure needed to provide access to data and supporting software. These documents, or compendiums, can be processed in a number of different ways. One transformation will be to replace the code with its output – thereby providing the familiar, but limited, static document.},
	language = {en},
	number = {1},
	journal = {Statistical Applications in Genetics and Molecular Biology},
	author = {Gentleman, Robert},
	month = jan,
	year = {2005},
	keywords = {done},
}

@article{gentleman_statistical_2007-2,
	title = {Statistical {Analyses} and {Reproducible} {Research}},
	volume = {16},
	doi = {10.1198/106186007X178663},
	abstract = {It is important, if not essential, to integrate the computations and code used in data analyses, methodological descriptions, simulations, and so on with the documents that describe and rely on them. This integration allows readers to both verify and adapt the claims in the documents. Authors can easily reproduce the results in the future, and they can present the document's contents in a different medium, for example, with interactive controls. This article describes a software framework for both authoring and distributing these integrated, dynamic documents that contain text, code, data, and any auxiliary content needed to recreate the computations. The documents are dynamic in that the contents— including figures, tables, and so on— can be recalculated each time a view of the document is generated. Our model treats a dynamic document as a master or “source” document from which one can generate different views in the form of traditional, derived documents for different audiences.We introduce the concept of a compendium as a container for one or more dynamic documents and the different elements needed when processing them, such as code and data. The compendium serves as a means for distributing, managing, and updating the collection.The step from disseminating analyses via a compendium to reproducible research is a small one. By reproducible research, we mean research papers with accompanying software tools that allow the reader to directly reproduce the results and employ the computational methods that are presented in the research paper. Some of the issues involved in paradigms for the production, distribution, and use of such reproducible research are discussed.},
	number = {1},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Gentleman, Robert and Lang, Duncan Temple},
	year = {2007},
	keywords = {done},
	pages = {1--23},
	annote = {Publisher: Taylor \& Francis \_eprint: https://doi.org/10.1198/106186007X178663},
}

@article{golub_molecular_1999-2,
	title = {Molecular {Classification} of {Cancer}: {Class} {Discovery} and {Class} {Prediction} by {Gene} {Expression} {Monitoring}},
	volume = {286},
	issn = {0036-8075},
	shorttitle = {Molecular {Classification} of {Cancer}},
	doi = {10.1126/science.286.5439.531},
	abstract = {Although cancer classification has improved over the past 30 years, there has been no general approach for identifying new cancer classes (class discovery) or for assigning tumors to known classes (class prediction). Here, a generic approach to cancer classification based on gene expression monitoring by DNA microarrays is described and applied to human acute leukemias as a test case. A class discovery procedure automatically discovered the distinction between acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL) without previous knowledge of these classes. An automatically derived class predictor was able to determine the class of new leukemia cases. The results demonstrate the feasibility of cancer classification based solely on gene expression monitoring and suggest a general strategy for discovering and predicting cancer classes for other types of cancer, independent of previous biological knowledge.},
	language = {eng},
	number = {5439},
	journal = {Science (New York, N.Y.)},
	author = {Golub, T. R. and Slonim, D. K. and Tamayo, P. and Huard, C. and Gaasenbeek, M. and Mesirov, J. P. and Coller, H. and Loh, M. L. and Downing, J. R. and Caligiuri, M. A. and Bloomfield, C. D. and Lander, E. S.},
	month = oct,
	year = {1999},
	pmid = {10521349},
	keywords = {done},
	pages = {531--537},
}

@article{goodman_what_2016-2,
	title = {What {Does} {Research} {Reproducibility} {Mean}?},
	volume = {8},
	issn = {1946-6234, 1946-6242},
	doi = {10.1126/scitranslmed.aaf5027},
	language = {en},
	number = {341},
	journal = {Science Translational Medicine},
	author = {Goodman, Steven N. and Fanelli, Daniele and Ioannidis, John P. A.},
	month = jun,
	year = {2016},
	keywords = {done},
	pages = {341ps12--341ps12},
}

@book{grolemund_r_nodate-2,
	title = {R for {Data} {Science}},
	abstract = {This book will teach you how to do data science with R: You'll learn how to get your data into R, get it into the most useful structure, transform it, visualise it and model it. In this book, you will find a practicum of skills for data science. Just as a chemist learns how to clean test tubes and stock a lab, you'll learn how to clean data and draw plots— and many other things besides. These are the skills that allow data science to happen, and here you will find the best practices for doing each of these things with R. You'll learn how to use the grammar of graphics, literate programming, and reproducible research to save time. You'll also learn how to manage cognitive resources to facilitate discoveries when wrangling, visualising, and exploring data.},
	author = {Grolemund, Garrett and Wickham, Hadley},
}

@misc{gruber_daring_nodate-4,
	title = {Daring {Fireball}: {Markdown}},
	url = {https://daringfireball.net/projects/markdown/},
	author = {Gruber, John and Swartz, Aron},
	keywords = {done},
}

@misc{gruber_daring_nodate-5,
	title = {Daring {Fireball}: {Markdown}},
	url = {https://daringfireball.net/projects/markdown/},
	author = {Gruber, John and Swartz, Aron},
	keywords = {done},
}

@article{ioannidis_why_2005-2,
	title = {Why {Most} {Published} {Research} {Findings} {Are} {False}},
	volume = {2},
	issn = {1549-1676},
	doi = {10.1371/journal.pmed.0020124},
	abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
	language = {en},
	number = {8},
	journal = {PLOS Medicine},
	author = {Ioannidis, John P. A.},
	month = aug,
	year = {2005},
	keywords = {done},
	pages = {e124},
	annote = {Publisher: Public Library of Science},
}

@article{iyengar_selection_1988-2,
	title = {Selection {Models} and the {File} {Drawer} {Problem}},
	volume = {3},
	issn = {08834237},
	abstract = {Meta-analysis consists of quantitative methods for combining evidence from different studies about a particular issue. A frequent criticism of meta-analysis is that it may be based on a biased sample of all studies that were done. In this paper, we use selection models, or weighted distributions, to deal with one source of bias, namely, the failure to report studies that do not yield statistically significant results. We apply selection models to two approaches that have been suggested for correcting the bias. The fail-safe sample size approach calculates the minimum number of unpublished studies showing nonsignificant results that must have been carried out in order to overturn the conclusion reached from the published studies. The maximum likelihood approach uses a weighted distribution to model the selection bias in the generation of the data and estimates various parameters of interest. We suggest the use of families of weight functions to model plausible biasing mechanisms to study the sensitivity of inferences about effect sizes. By using an example, we show that the maximum likelihood approach has several advantages over the fail-safe sample size approach.},
	number = {1},
	journal = {Statistical Science},
	author = {Iyengar, Satish and Greenhouse, Joel B.},
	year = {1988},
	keywords = {done},
	pages = {109--117},
	annote = {Publisher: Institute of Mathematical Statistics},
}

@article{jasny_again_2011-2,
	title = {Again, and {Again}, and {Again}},
	volume = {334},
	issn = {0036-8075},
	doi = {10.1126/science.334.6060.1225},
	number = {6060},
	journal = {Science},
	author = {Jasny, Barbara R. and Chin, Gilbert and Chong, Lisa and Vignieri, Sacha},
	year = {2011},
	keywords = {done},
	pages = {1225--1225},
	annote = {Publisher: American Association for the Advancement of Science \_eprint: https://science.sciencemag.org/content/334/6060/1225.full.pdf},
}

@article{klein_many_2018-2,
	title = {Many {Labs} 2: {Investigating} {Variation} in {Replicability} across {Samples} and {Settings}},
	volume = {1},
	doi = {10.1177/2515245918810225},
	abstract = {We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p ¡ .05), we found that 15 (54\%) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p ¡ .0001), 14 (50\%) of the replications still provided such evidence, a reflection of the extremely high-powered design. Seven (25\%) of the replications yielded effect sizes larger than the original ones, and 21 (75\%) yielded effect sizes smaller than the original ones. The median comparable Cohen's ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small (¡ 0.20) in 16 of the replications (57\%), and 9 effects (32\%) were in the direction opposite the direction of the original effect. Across settings, the Q statistic indicated significant heterogeneity in 11 (39\%) of the replication effects, and most of those were among the findings with the largest overall effect sizes; only 1 effect that was near zero in the aggregate showed significant heterogeneity according to this measure. Only 1 effect had a tau value greater than .20, an indication of moderate heterogeneity. Eight others had tau values near or slightly above .10, an indication of slight heterogeneity. Moderation tests indicated that very little heterogeneity was attributable to the order in which the tasks were performed or whether the tasks were administered in lab versus online. Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized, rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness scores, respectively). Cumulatively, variability in the observed effect sizes was attributable more to the effect being studied than to the sample or setting in which it was studied.},
	number = {4},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Klein, Richard A. and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G. and Reginald B. Adams, Jr. and Alper, Sinan and Aveyard, Mark and Axt, Jordan R. and Babalola, Mayowa T. and Bahník, Štěpán and Batra, Rishtee and Berkics, Mihály and Bernstein, Michael J. and Berry, Daniel R. and Bialobrzeska, Olga and Binan, Evans Dami and Bocian, Konrad and Brandt, Mark J. and Busching, Robert and Rédei, Anna Cabak and Cai, Huajian and Cambier, Fanny and Cantarero, Katarzyna and Carmichael, Cheryl L. and Ceric, Francisco and Chandler, Jesse and Chang, Jen-Ho and Chatard, Armand and Chen, Eva E. and Cheong, Winnee and Cicero, David C. and Coen, Sharon and Coleman, Jennifer A. and Collisson, Brian and Conway, Morgan A. and Corker, Katherine S. and Curran, Paul G. and Cushman, Fiery and Dagona, Zubairu K. and Dalgar, Ilker and Rosa, Anna Dalla and Davis, William E. and de Bruijn, Maaike and Schutter, Leander De and Devos, Thierry and de Vries, Marieke and Doğulu, Canay and Dozo, Nerisa and Dukes, Kristin Nicole and Dunham, Yarrow and Durrheim, Kevin and Ebersole, Charles R. and Edlund, John E. and Eller, Anja and English, Alexander Scott and Finck, Carolyn and Frankowska, Natalia and Freyre, Miguel-Ángel and Friedman, Mike and Galliani, Elisa Maria and Gandi, Joshua C. and Ghoshal, Tanuka and Giessner, Steffen R. and Gill, Tripat and Gnambs, Timo and Gómez, Ángel and González, Roberto and Graham, Jesse and Grahe, Jon E. and Grahek, Ivan and Green, Eva G. T. and Hai, Kakul and Haigh, Matthew and Haines, Elizabeth L. and Hall, Michael P. and Heffernan, Marie E. and Hicks, Joshua A. and Houdek, Petr and Huntsinger, Jeffrey R. and Huynh, Ho Phi and IJzerman, Hans and Inbar, Yoel and Innes-Ker, Åse H. and Jiménez-Leal, William and John, Melissa-Sue and Joy-Gaba, Jennifer A. and Kamiloğlu, Roza G. and Kappes, Heather Barry and Karabati, Serdar and Karick, Haruna and Keller, Victor N. and Kende, Anna and Kervyn, Nicolas and Knežević, Goran and Kovacs, Carrie and Krueger, Lacy E. and Kurapov, German and Kurtz, Jamie and Lakens, Daniël and Lazarević, Ljiljana B. and Levitan, Carmel A. and Neil A. Lewis, Jr. and Lins, Samuel and Lipsey, Nikolette P. and Losee, Joy E. and Maassen, Esther and Maitner, Angela T. and Malingumu, Winfrida and Mallett, Robyn K. and Marotta, Satia A. and Me{\textbackslash}textbackslashdjedović, Janko and Mena-Pacheco, Fernando and Milfont, Taciano L. and Morris, Wendy L. and Murphy, Sean C. and Myachykov, Andriy and Neave, Nick and Neijenhuijs, Koen and Nelson, Anthony J. and Neto, Félix and Nichols, Austin Lee and Ocampo, Aaron and O'Donnell, Susan L. and Oikawa, Haruka and Oikawa, Masanori and Ong, Elsie and Orosz, Gábor and Osowiecka, Malgorzata and Packard, Grant and Pérez-Sánchez, Rolando and Petrović, Boban and Pilati, Ronaldo and Pinter, Brad and Podesta, Lysandra and Pogge, Gabrielle and Pollmann, Monique M. H. and Rutchick, Abraham M. and Saavedra, Patricio and Saeri, Alexander K. and Salomon, Erika and Schmidt, Kathleen and Schönbrodt, Felix D. and Sekerdej, Maciej B. and Sirlopú, David and Skorinko, Jeanine L. M. and Smith, Michael A. and Smith-Castro, Vanessa and Smolders, Karin C. H. J. and Sobkow, Agata and Sowden, Walter and Spachtholz, Philipp and Srivastava, Manini and Steiner, Troy G. and Stouten, Jeroen and Street, Chris N. H. and Sundfelt, Oskar K. and Szeto, Stephanie and Szumowska, Ewa and Tang, Andrew C. W. and Tanzer, Norbert and Tear, Morgan J. and Theriault, Jordan and Thomae, Manuela and Torres, David and Traczyk, Jakub and Tybur, Joshua M. and Ujhelyi, Adrienn and van Aert, Robbie C. M. and van Assen, Marcel A. L. M. and van der Hulst, Marije and van Lange, Paul A. M. and van 't Veer, Anna Elisabeth and Echeverría, Alejandro Vásquez- and Vaughn, Leigh Ann and Vázquez, Alexandra and Vega, Luis Diego and Verniers, Catherine and Verschoor, Mark and Voermans, Ingrid P. J. and Vranka, Marek A. and Welch, Cheryl and Wichman, Aaron L. and Williams, Lisa A. and Wood, Michael and Woodzicka, Julie A. and Wronska, Marta K. and Young, Liane and Zelenski, John M. and Zhijia, Zeng and Nosek, Brian A.},
	year = {2018},
	keywords = {done},
	pages = {443--490},
	annote = {\_eprint: https://doi.org/10.1177/2515245918810225},
}

@article{knuth_literate_1984-2,
	title = {Literate {Programming}},
	volume = {27},
	issn = {0010-4620},
	doi = {10.1093/comjnl/27.2.97},
	abstract = {The author and his associates have been experimenting for the past several years with a programming language and documentation system called WEB. This paper presents WEB by example, and discusses why the new system appears to be an improvement over previous ones.},
	number = {2},
	journal = {The Computer Journal},
	author = {Knuth, D. E.},
	month = jan,
	year = {1984},
	keywords = {done},
	pages = {97--111},
	annote = {\_eprint: https://academic.oup.com/comjnl/article-pdf/27/2/97/981657/270097.pdf},
}

@book{knuth_texbook_1986-2,
	series = {Computers \& {Typesetting}},
	title = {The {TeXbook}},
	isbn = {978-0-201-13447-6},
	publisher = {Addison-Wesley},
	author = {Knuth, D.E.},
	year = {1986},
	lccn = {85030845},
	keywords = {done},
}

@book{knuth_literate_1992-2,
	title = {Literate {Programming}},
	isbn = {978-0-937073-81-0},
	abstract = {Literate programming is a programming methodology that combines a programming language with a documentation language, making programs more robust, more portable, and more easily maintained than programs written only in a high-level language. Computer programmers already know both kinds of languages; they need only learn a few conventions about alternating between languages to create programs that are works of literature. A literate programmer is an essayist who writes programs for humans to understand, instead of primarily writing instructions for machines to follow. When programs are written in the recommended style they can be transformed into documents by a document compiler and into efficient code by an algebraic compiler. This anthology of essays from the inventor of literate programming includes Knuth's early papers on related topics such as structured programming, as well as the Computer Journal article that launched literate programming itself.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Knuth, Donald E.},
	month = mar,
	year = {1992},
	keywords = {done},
}

@book{lander_r_2017-2,
	address = {Boston},
	edition = {2nd Edition},
	title = {R for {Everyone}: {Advanced} {Analytics} and {Graphics}},
	isbn = {978-0-13-454692-6},
	shorttitle = {R for {Everyone}},
	abstract = {Statistical Computation for Programmers, Scientists, Quants, Excel Users, and Other Professionals Using the open source R language, you can build powerful statistical models to answer many of your most challenging questions. R has traditionally been difficult for non-statisticians to learn, and most R books assume far too much knowledge to be of help. R for Everyone, Second Edition, is the solution. Drawing on his unsurpassed experience teaching new users, professional data scientist Jared P. Lander has written the perfect tutorial for anyone new to statistical programming and modeling. Organized to make learning easy and intuitive, this guide focuses on the 20 percent of R functionality you'll need to accomplish 80 percent of modern data tasks. Lander's self-contained chapters start with the absolute basics, offering extensive hands-on practice and sample code. You'll download and install R; navigate and use the R environment; master basic program control, data import, manipulation, and visualization; and walk through several essential tests. Then, building on this foundation, you'll construct several complete models, both linear and nonlinear, and use some data mining techniques. After all this you'll make your code reproducible with LaTeX, RMarkdown, and Shiny. By the time you're done, you won't just know how to write R programs, you'll be ready to tackle the statistical problems you care about most. Coverage includes Explore R, RStudio, and R packages Use R for math: variable types, vectors, calling functions, and more Exploit data structures, including data.frames, matrices, and lists Read many different types of data Create attractive, intuitive statistical graphics Write user-defined functions Control program flow with if, ifelse, and complex checks Improve program efficiency with group manipulations Combine and reshape multiple datasets Manipulate strings using R's facilities and regular expressions Create normal, binomial, and Poisson probability distributions Build linear, generalized linear, and nonlinear models Program basic statistics: mean, standard deviation, and t-tests Train machine learning models Assess the quality of models and variable selection Prevent overfitting and perform variable selection, using the Elastic Net and Bayesian methods Analyze univariate and multivariate time series data Group data via K-means and hierarchical clustering Prepare reports, slideshows, and web pages with knitr Display interactive data with RMarkdown and htmlwidgets Implement dashboards with Shiny Build reusable R packages with devtools and Rcpp Register your product at informit.com/register for convenient access to downloads, updates, and corrections as they become available.},
	language = {English},
	publisher = {Addison-Wesley Professional},
	author = {Lander, Jared P.},
	month = jun,
	year = {2017},
}

@incollection{leisch_sweave_2002-2,
	address = {Heidelberg},
	title = {Sweave: {Dynamic} {Generation} of {Statistical} {Reports} {Using} {Literate} {Data} {Analysis}},
	isbn = {978-3-7908-1517-7 978-3-642-57489-4},
	shorttitle = {Sweave},
	abstract = {Sweave combines typesetting with LATEX and data anlysis with S into integrated statistical documents. When run through R or Splus, all data analysis output (tables, graphs, . . . ) is created on the fly and inserted into a final LATEX document. Options control which parts of the original S code are shown to or hidden from the reader, respectively. Many S users are also LATEX users, hence no new software has to be learned. The report can be automatically updated if data or analysis change, which allows for truly reproducible research.},
	language = {en},
	booktitle = {Compstat},
	publisher = {Physica-Verlag HD},
	author = {Leisch, Friedrich},
	editor = {Härdle, Wolfgang and Rönz, Bernd},
	year = {2002},
	doi = {10.1007/978-3-642-57489-4_89},
	keywords = {done},
	pages = {575--580},
}

@article{markowetz_five_2015-2,
	title = {Five {Selfish} {Reasons} to {Work} {Reproducibly}},
	volume = {16},
	issn = {1474-760X},
	doi = {10.1186/s13059-015-0850-7},
	language = {en},
	number = {1},
	journal = {Genome Biology},
	author = {Markowetz, Florian},
	month = dec,
	year = {2015},
	pages = {274},
}

@article{mccullough_economics_2008-2,
	title = {Do {Economics} {Journal} {Archives} {Promote} {Replicable} {Research}?},
	volume = {41},
	copyright = {© Canadian Economics Association},
	issn = {1540-5982},
	doi = {10.1111/j.1540-5982.2008.00509.x},
	abstract = {Abstract. All the long-standing archives at economics journals do not facilitate the reproduction of published results. The data-only archives at Journal of Business and Economic Statistics and Economic Journal fail in part because most authors do not contribute data. Results published in the FRB St. Louis Review can rarely be reproduced using the data+code in the journal archive. Recently created archives at top journals should avoid the mistakes of their predecessors. We categorize reasons for archives' failures and identify successful policies.},
	language = {en},
	number = {4},
	journal = {Canadian Journal of Economics/Revue canadienne d'économique},
	author = {McCullough, B. D. and McGeary, Kerry Anne and Harrison, Teresa D.},
	year = {2008},
	keywords = {done},
	pages = {1406--1420},
}

@article{mcnutt_reproducibility_2014-2,
	title = {Reproducibility},
	volume = {343},
	issn = {0036-8075},
	doi = {10.1126/science.1250475},
	number = {6168},
	journal = {Science},
	author = {McNutt, Marcia},
	year = {2014},
	keywords = {done},
	pages = {229--229},
	annote = {Publisher: American Association for the Advancement of Science \_eprint: https://science.sciencemag.org/content/343/6168/229.full.pdf},
}

@article{nosek_promoting_2015-2,
	title = {Promoting an {Open} {Research} {Culture}},
	volume = {348},
	issn = {0036-8075, 1095-9203},
	doi = {10.1126/science.aab2374},
	language = {en},
	number = {6242},
	journal = {Science},
	author = {Nosek, B. A. and Alter, G. and Banks, G. C. and Borsboom, D. and Bowman, S. D. and Breckler, S. J. and Buck, S. and Chambers, C. D. and Chin, G. and Christensen, G. and Contestabile, M. and Dafoe, A. and Eich, E. and Freese, J. and Glennerster, R. and Goroff, D. and Green, D. P. and Hesse, B. and Humphreys, M. and Ishiyama, J. and Karlan, D. and Kraut, A. and Lupia, A. and Mabry, P. and Madon, T. and Malhotra, N. and Mayo-Wilson, E. and McNutt, M. and Miguel, E. and Paluck, E. L. and Simonsohn, U. and Soderberg, C. and Spellman, B. A. and Turitto, J. and VandenBos, G. and Vazire, S. and Wagenmakers, E. J. and Wilson, R. and Yarkoni, T.},
	month = jun,
	year = {2015},
	keywords = {done},
	pages = {1422--1425},
}

@article{nust_opening_2017-2,
	title = {Opening the {Publication} {Process} with {Executable} {Research} {Compendia}},
	volume = {23},
	issn = {1082-9873},
	doi = {10.1045/january2017-nuest},
	language = {en},
	number = {1/2},
	journal = {D-Lib Magazine},
	author = {Nüst, Daniel and Konkol, Markus and Pebesma, Edzer and Kray, Christian and Schutzeichel, Marc and Przibytzin, Holger and Lorenz, Jörg},
	month = jan,
	year = {2017},
}

@article{collaboration_estimating_2015-2,
	title = {Estimating the {Reproducibility} of {Psychological} {Science}},
	volume = {349},
	issn = {0036-8075},
	doi = {10.1126/science.aac4716},
	abstract = {One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.Science, this issue 10.1126/science.aac4716INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P \&lt; .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that we already know this belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
	number = {6251},
	journal = {Science},
	author = {Collaboration, Open Science},
	year = {2015},
	keywords = {done},
	annote = {Publisher: American Association for the Advancement of Science \_eprint: https://science.sciencemag.org/content/349/6251/aac4716.full.pdf},
}

@article{peng_reproducible_2011-2,
	title = {Reproducible {Research} in {Computational} {Science}},
	volume = {334},
	copyright = {Copyright © 2011, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	doi = {10.1126/science.1213847},
	abstract = {Computational science has led to exciting new developments, but the nature of the work has exposed limitations in our ability to evaluate published findings. Reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible.},
	language = {en},
	number = {6060},
	journal = {Science},
	author = {Peng, Roger D.},
	month = dec,
	year = {2011},
	pmid = {22144613},
	keywords = {done},
	pages = {1226--1227},
	annote = {Publisher: American Association for the Advancement of Science Section: Perspective},
}

@article{ram_git_2013-2,
	title = {Git {Can} {Facilitate} {Greater} {Reproducibility} and {Increased} {Transparency} in {Science}},
	volume = {8},
	issn = {1751-0473},
	doi = {10.1186/1751-0473-8-7},
	abstract = {Reproducibility is the hallmark of good science. Maintaining a high degree of transparency in scientific reporting is essential not just for gaining trust and credibility within the scientific community but also for facilitating the development of new ideas. Sharing data and computer code associated with publications is becoming increasingly common, motivated partly in response to data deposition requirements from journals and mandates from funders. Despite this increase in transparency, it is still difficult to reproduce or build upon the findings of most scientific publications without access to a more complete workflow.},
	number = {1},
	journal = {Source Code for Biology and Medicine},
	author = {Ram, Karthik},
	month = feb,
	year = {2013},
	pages = {7},
}

@misc{ramsey_noweb_nodate-2,
	title = {Noweb {Home} {Page}},
	url = {https://www.cs.tufts.edu/\textasciitilde nr/noweb/},
	author = {Ramsey, Norman},
	keywords = {done},
}

@book{r_core_team_r_2020-2,
	address = {Vienna, Austria},
	title = {R: {A} {Language} and {Environment} for {Statistical} {Computing}},
	publisher = {R Foundation for Statistical Computing},
	author = {{R Core Team}},
	year = {2020},
	keywords = {done},
}

@book{riederer_r_nodate-2,
	title = {R {Markdown} {Cookbook}},
	abstract = {This book showcases short, practical examples of lesser-known tips and tricks to helps users get the most out of these tools. After reading this book, you will understand how R Markdown documents are transformed from plain text and how you may customize nearly every step of this processing. For example, you will learn how to dynamically create content from R code, reference code in other documents or chunks, control the formatting with customer templates, fine-tune how your code is processed, and incorporate multiple languages into your analysis.},
	author = {Riederer, Christophe Dervieux, Emily, Yihui Xie},
	keywords = {done},
}

@inproceedings{rosenthal_file_1979-2,
	title = {The {File} {Drawer} {Problem} and {Tolerance} for {Null} {Results}.},
	volume = {86},
	publisher = {Psychological Bulletin},
	author = {Rosenthal, R.},
	year = {1979},
	keywords = {done},
	pages = {638--641},
}

@book{rstudio_team_rstudio_2020-2,
	address = {Boston, MA},
	title = {{RStudio}: {Integrated} {Development} {Environment} for r},
	publisher = {RStudio, PBC.},
	author = {{RStudio Team}},
	year = {2020},
}

@article{schwab_reproducible_1995-2,
	title = {Reproducible {Electronic} {Documents}},
	abstract = {To organize computational scientific research and hence to conveniently transfer our technology, we impose a simple filing discipline on the authors in our laboratory. A document's makefile includes laboratory-wide standard rules that offer readers these four standard commands: make burn removes the document's result figures, make build recomputes them, make view displays the figures, and make clean removes any intermediate files. Although we developed these standards to aid readers we discovered that authors are often the principal beneficiaries.},
	language = {en},
	author = {Schwab, Matthias and Karrenbach, Martin and Claerbout, Jon},
	year = {1995},
	keywords = {done},
	pages = {14},
}

@article{simmons_false-positive_2011-2,
	title = {False-{Positive} {Psychology}: {Undisclosed} {Flexibility} in {Data} {Collection} and {Analysis} {Allows} {Presenting} {Anything} as {Significant}},
	volume = {22},
	doi = {10.1177/0956797611417632},
	abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings (\${\textbackslash}textbackslashleq\$ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
	number = {11},
	journal = {Psychological Science},
	author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
	year = {2011},
	pmid = {22006061},
	keywords = {done},
	pages = {1359--1366},
	annote = {\_eprint: https://doi.org/10.1177/0956797611417632},
}

@book{tierney_rmarkdown_nodate-2,
	title = {{RMarkdown} for {Scientists}},
	abstract = {A book created for a 3 hour workshop on rmarkdown},
	author = {Tierney, Nicholas},
	keywords = {done},
}

@book{wickham_r_2016-2,
	address = {Beijing},
	title = {R for {Data} {Science}: {Import}, {Tidy}, {Transform}, {Visualize}, and {Model} {Data}},
	isbn = {978-1-4919-1039-9},
	shorttitle = {R for {Data} {Science}},
	language = {eng},
	publisher = {O'Reilly},
	author = {Wickham, Hadley and Grolemund, Garrett},
	year = {2016},
	lccn = {006.312 Wic, 519.2 W63r, 006.312 WIC},
}

@article{wikipedia_meta-analysis_2020-2,
	title = {Meta-{Analysis}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	abstract = {A meta-analysis is a statistical analysis that combines the results of multiple scientific studies. Meta-analysis can be performed when there are multiple scientific studies addressing the same question, with each individual study reporting measurements that are expected to have some degree of error. The aim then is to use approaches from statistics to derive a pooled estimate closest to the unknown common truth based on how this error is perceived. Existing methods for meta-analysis yield a weighted average from the results of the individual studies, and what differs is the manner in which these weights are allocated and also the manner in which the uncertainty is computed around the point estimate thus generated. In addition to providing an estimate of the unknown common truth, meta-analysis has the capacity to contrast results from different studies and identify patterns among study results, sources of disagreement among those results, or other interesting relationships that may come to light in the context of multiple studies.A key benefit of this approach is the aggregation of information leading to a higher statistical power and more robust point estimate than is possible from the measure derived from any individual study. However, in performing a meta-analysis, an investigator must make choices which can affect the results, including deciding how to search for studies, selecting studies based on a set of objective criteria, dealing with incomplete data, analyzing the data, and accounting for or choosing not to account for publication bias. Judgment calls made in completing a meta-analysis may affect the results. For example, Wanous and colleagues examined four pairs of meta-analyses on the four topics of (a) job performance and satisfaction relationship, (b) realistic job previews, (c) correlates of role conflict and ambiguity, and (d) the job satisfaction and absenteeism relationship, and illustrated how various judgement calls made by the researchers produced different results.Meta-analyses are often, but not always, important components of a systematic review procedure. For instance, a meta-analysis may be conducted on several clinical trials of a medical treatment, in an effort to obtain a better understanding of how well the treatment works. Here it is convenient to follow the terminology used by the Cochrane Collaboration, and use "meta-analysis" to refer to statistical methods of combining evidence, leaving other aspects of 'research synthesis' or 'evidence synthesis', such as combining information from qualitative studies, for the more general context of systematic reviews. A meta-analysis is a secondary source.},
	language = {en},
	journal = {Wikipedia},
	author = {{wikipedia}},
	month = aug,
	year = {2020},
	keywords = {done},
}

@incollection{xie_knitr_2014-2,
	title = {Knitr: {A} {Comprehensive} {Tool} for {Reproducible} {Research} in {R}},
	booktitle = {Implementing {Reproducible} {Computational} {Research}},
	publisher = {Chapman and Hall/CRC},
	author = {Xie, Yihui},
	editor = {Stodden, Victoria and Leisch, Friedrich and Peng, Roger D.},
	year = {2014},
	keywords = {done},
	annote = {ISBN 978-1466561595},
}

@book{xie_dynamic_2015-2,
	address = {Boca Raton, Florida},
	edition = {Second},
	title = {Dynamic {Documents} with {R} and {Knitr}},
	publisher = {Chapman and Hall/CRC},
	author = {Xie, Yihui},
	year = {2015},
	keywords = {done},
	annote = {ISBN 978-1498716963},
}

@book{xie_r_2018-2,
	address = {Boca Raton, Florida},
	title = {R {Markdown}: {The} {Definitive} {Guide}},
	publisher = {Chapman and Hall/CRC},
	author = {Xie, Yihui and Allaire, J.J. and Grolemund, Garrett},
	year = {2018},
	keywords = {done},
	annote = {ISBN 9781138359338},
}

@book{xie_knitr_2020-2,
	title = {Knitr: {A} {General}-{Purpose} {Package} for {Dynamic} {Report} {Generation} in r},
	author = {Xie, Yihui},
	year = {2020},
	keywords = {done},
	annote = {R package version 1.29},
	annote = {Type: Manual},
}

@article{young_why_2008-2,
	title = {Why {Current} {Publication} {Practices} {May} {Distort} {Science}},
	volume = {5},
	issn = {1549-1676},
	doi = {10.1371/journal.pmed.0050201},
	abstract = {The current system of publication in biomedical research provides a distorted view of the reality of scientific data that are generated in the laboratory and clinic. This system can be studied by applying principles from the field of economics. The “winner's curse,” a more general statement of publication bias, suggests that the small proportion of results chosen for publication are unrepresentative of scientists' repeated samplings of the real world. The self-correcting mechanism in science is retarded by the extreme imbalance between the abundance of supply (the output of basic science laboratories and clinical investigations) and the increasingly limited venues for publication (journals with sufficiently high impact). This system would be expected intrinsically to lead to the misallocation of resources. The scarcity of available outlets is artificial, based on the costs of printing in an electronic age and a belief that selectivity is equivalent to quality. Science is subject to great uncertainty: we cannot be confident now which efforts will ultimately yield worthwhile achievements. However, the current system abdicates to a small number of intermediates an authoritative prescience to anticipate a highly unpredictable future. In considering society's expectations and our own goals as scientists, we believe that there is a moral imperative to reconsider how scientific data are judged and disseminated.},
	language = {en},
	number = {10},
	journal = {PLoS Medicine},
	author = {Young, Neal S and Ioannidis, John P. A and Al-Ubaydli, Omar},
	month = oct,
	year = {2008},
	keywords = {done},
	pages = {e201},
}

@misc{noauthor_dear_nodate-4,
	title = {Dear {Colleague} {Letter}: {Robust} and {Reliable} {Research} in the {Social}, {Behavioral}, and {Economic} {Sciences} ({Nsf16137}) {\textbackslash}textbar {NSF} - {National} {Science} {Foundation}},
	url = {https://www.nsf.gov/pubs/2016/nsf16137/nsf16137.jsp},
	keywords = {done},
}

@misc{noauthor_dear_nodate-5,
	title = {Dear {Colleague} {Letter}: {Robust} and {Reliable} {Research} in the {Social}, {Behavioral}, and {Economic} {Sciences} ({Nsf16137}) {\textbackslash}textbar {NSF} - {National} {Science} {Foundation}},
	url = {https://www.nsf.gov/pubs/2016/nsf16137/nsf16137.jsp},
	keywords = {done},
}

@misc{noauthor_american_nodate-2,
	title = {American {Economic} {Association}},
	url = {https://www.aeaweb.org/journals/data},
}

@misc{noauthor_pandoc_nodate-2,
	title = {Pandoc - {Pandoc} {User}'s {Guide}},
	url = {https://pandoc.org/MANUAL.html},
}

@article{nosek_estimating_2015-2,
	title = {Estimating the {Reproducibility} of {Psychological} {Science}},
	volume = {349},
	issn = {0036-8075},
	doi = {10.1126/science.aac4716},
	abstract = {One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.Science, this issue 10.1126/science.aac4716INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P \&lt; .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that we already know this belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
	number = {6251},
	journal = {Science},
	author = {Nosek, Brian A. and {et al}},
	year = {2015},
	keywords = {done},
	annote = {Publisher: American Association for the Advancement of Science \_eprint: https://science.sciencemag.org/content/349/6251/aac4716.full.pdf},
}

@book{allaire_rmarkdown_2020-3,
	title = {Rmarkdown: {Dynamic} {Documents} for r},
	author = {Allaire, JJ and Xie, Yihui and McPherson, Jonathan and Luraschi, Javier and Ushey, Kevin and Atkins, Aron and Wickham, Hadley and Cheng, Joe and Chang, Winston and Iannone, Richard},
	year = {2020},
	keywords = {done},
	annote = {R package version 2.2},
}

@article{barnes_publish_2010-3,
	title = {Publish {Your} {Computer} {Code}: {It} {Is} {Good} {Enough}},
	volume = {467},
	issn = {0028-0836, 1476-4687},
	shorttitle = {Publish {Your} {Computer} {Code}},
	doi = {10.1038/467753a},
	language = {en},
	number = {7317},
	journal = {Nature},
	author = {Barnes, Nick},
	month = oct,
	year = {2010},
	pages = {753--753},
}

@article{bechhofer_why_2013-3,
	title = {Why {Linked} {Data} {Is} {Not} {Enough} for {Scientists}},
	volume = {29},
	issn = {0167739X},
	doi = {10.1016/j.future.2011.08.004},
	language = {en},
	number = {2},
	journal = {Future Generation Computer Systems},
	author = {Bechhofer, Sean and Buchan, Iain and De Roure, David and Missier, Paolo and Ainsworth, John and Bhagat, Jiten and Couch, Philip and Cruickshank, Don and Delderfield, Mark and Dunlop, Ian and Gamble, Matthew and Michaelides, Danius and Owen, Stuart and Newman, David and Sufi, Shoaib and Goble, Carole},
	month = feb,
	year = {2013},
	keywords = {done},
	pages = {599--611},
}

@techreport{bollen_social_2015-3,
	title = {Social, {Behavioral}, and {Economic} {Sciences} {Perspectives} on {Robust} and {Reliable} {Science}},
	number = {Report of the Subcommittee on Replicability in Science Advisory Committee to the National Science Foundation Directorate for Social, Behavioral, and Economic Sciences},
	institution = {NSF},
	author = {Bollen, Kenneth and Cacioppo, John T. and Krosnick, Jon A. and Olds, James L. and Kaplan, Robert M.},
	year = {2015},
	keywords = {done},
}

@inproceedings{brase_datacite_2009-3,
	address = {Beijing, China},
	title = {{DataCite} - {A} {Global} {Registration} {Agency} for {Research} {Data}},
	isbn = {978-0-7695-3898-3},
	doi = {10.1109/COINFO.2009.66},
	booktitle = {2009 {Fourth} {International} {Conference} on {Cooperation} and {Promotion} of {Information} {Resources} in {Science} and {Technology}},
	publisher = {IEEE},
	author = {Brase, Jan},
	month = nov,
	year = {2009},
	keywords = {done},
	pages = {257--261},
}

@incollection{buckheit_wavelab_1995-3,
	address = {New York, NY},
	title = {{WaveLab} and {Reproducible} {Research}},
	volume = {103},
	isbn = {978-0-387-94564-4 978-1-4612-2544-7},
	abstract = {WaveLab is a library of Matlab routines for wavelet analysis, wavelet-packet analysis, cosine-packet analysis and matching pursuit. The library is available free of charge over the Internet. Versions are provided for Macintosh, UNIX and Windows machines.},
	language = {en},
	booktitle = {Wavelets and {Statistics}},
	publisher = {Springer New York},
	author = {Buckheit, Jonathan B. and Donoho, David L.},
	editor = {Bickel, P. and Diggle, P. and Fienberg, S. and Krickeberg, K. and Olkin, I. and Wermuth, N. and Zeger, S. and Antoniadis, Anestis and Oppenheim, Georges},
	year = {1995},
	doi = {10.1007/978-1-4612-2544-7_5},
	keywords = {done},
	pages = {55--81},
}

@misc{conyers_learn_nodate-3,
	title = {Learn {Git} in 3 {Hours}},
	url = {https://www.safaribooksonline.com/videos/learn-git-in/9781789348231/9781789348231-video1_2},
	abstract = {Build powerful and effective projects using Git Version Control Systems About This Video Learn how to create, contribute to, and collaborate on software projects using Git Understand its fundamental features,...},
	author = {Conyers, Ross},
	annote = {Publication Title: O'Reilly {\textbackslash}textbar Safari},
}

@article{dewald_replication_1986-3,
	title = {Replication in {Empirical} {Economics}: {The} {Journal} of {Money}, {Credit} and {Banking} {Project}},
	volume = {76},
	issn = {0002-8282},
	shorttitle = {Replication in {Empirical} {Economics}},
	abstract = {This paper examines the role of replication in empirical economic research. It presents the findings of a two-year study that collected programs and data from authors and attempted to replicate their published results. Our research provides new and important information about the extent and causes of failures to replicate published results in economics. Our findings suggest that inadvertent errors in published empirical articles are a commonplace rather thana rare occurrence.},
	number = {4},
	journal = {The American Economic Review},
	author = {Dewald, William G. and Thursby, Jerry G. and Anderson, Richard G.},
	year = {1986},
	keywords = {done},
	pages = {587--603},
	annote = {Publisher: American Economic Association},
}

@article{dewald_replication_2020-3,
	title = {Replication in {Empirical} {Economics}: {The} {Journal} of {Money}, {Credit} and {Banking} {Project}},
	language = {en},
	author = {Dewald, William G and Thursby, Jerry G and Anderson, Richard G},
	year = {2020},
	keywords = {done},
	pages = {18},
}

@book{noauthor_access_2015-3,
	address = {LU},
	title = {Access to and {Preservation} of {Scientific} {Information} in {Europe}: {Report} on the {Implementation} of {Commission} {Recommendation} {C}(2012) 4890 {Final}.},
	shorttitle = {Access to and {Preservation} of {Scientific} {Information} in {Europe}},
	language = {eng},
	publisher = {Publications Office},
	year = {2015},
	keywords = {done},
}

@article{ezekiel_considerations_1933-3,
	title = {Some {Considerations} on the {Analysis} of the {Prices} of {Competing} or {Substitute} {Commodities}},
	volume = {1},
	issn = {00129682, 14680262},
	number = {2},
	journal = {Econometrica},
	author = {Ezekiel, Mordecai},
	year = {1933},
	keywords = {done},
	pages = {172--180},
	annote = {Publisher: [Wiley, Econometric Society]},
}

@article{fomel_guest_2009-3,
	title = {Guest {Editors}' {Introduction}: {Reproducible} {Research}},
	volume = {11},
	number = {1},
	journal = {Computing in Science Engineering},
	author = {Fomel, S. and Claerbout, J. F.},
	year = {2009},
	keywords = {done},
	pages = {5--7},
}

@article{frisch_editors_1933-3,
	title = {Editor's {Note}},
	volume = {1},
	issn = {00129682, 14680262},
	number = {1},
	journal = {Econometrica},
	author = {Frisch, Ragnar},
	year = {1933},
	keywords = {done},
	pages = {1--4},
	annote = {Publisher: [Wiley, Econometric Society]},
}

@article{gentleman_reproducible_2005-3,
	title = {Reproducible {Research}: {A} {Bioinformatics} {Case} {Study}},
	volume = {4},
	issn = {1544-6115, 2194-6302},
	shorttitle = {Reproducible {Research}},
	doi = {10.2202/1544-6115.1034},
	abstract = {While scientific research and the methodologies involved have gone through substantial technological evolution the technology involved in the publication of the results of these endeavors has remained relatively stagnant. Publication is largely done in the same manner today as it was fifty years ago. Many journals have adopted electronic formats, however, their orientation and style is little different from a printed document. The documents tend to be static and take little advantage of computational resources that might be available. Recent work, Gentleman and Temple Lang (2004), suggests a methodology and basic infrastructure that can be used to publish documents in a substantially different way. Their approach is suitable for the publication of papers whose message relies on computation. Stated quite simply, Gentleman and Temple Lang propose a paradigm where documents are mixtures of code and text. Such documents may be self-contained or they may be a component of a compendium which provides the infrastructure needed to provide access to data and supporting software. These documents, or compendiums, can be processed in a number of different ways. One transformation will be to replace the code with its output – thereby providing the familiar, but limited, static document.},
	language = {en},
	number = {1},
	journal = {Statistical Applications in Genetics and Molecular Biology},
	author = {Gentleman, Robert},
	month = jan,
	year = {2005},
	keywords = {done},
}

@article{gentleman_statistical_2007-3,
	title = {Statistical {Analyses} and {Reproducible} {Research}},
	volume = {16},
	doi = {10.1198/106186007X178663},
	abstract = {It is important, if not essential, to integrate the computations and code used in data analyses, methodological descriptions, simulations, and so on with the documents that describe and rely on them. This integration allows readers to both verify and adapt the claims in the documents. Authors can easily reproduce the results in the future, and they can present the document's contents in a different medium, for example, with interactive controls. This article describes a software framework for both authoring and distributing these integrated, dynamic documents that contain text, code, data, and any auxiliary content needed to recreate the computations. The documents are dynamic in that the contents— including figures, tables, and so on— can be recalculated each time a view of the document is generated. Our model treats a dynamic document as a master or “source” document from which one can generate different views in the form of traditional, derived documents for different audiences.We introduce the concept of a compendium as a container for one or more dynamic documents and the different elements needed when processing them, such as code and data. The compendium serves as a means for distributing, managing, and updating the collection.The step from disseminating analyses via a compendium to reproducible research is a small one. By reproducible research, we mean research papers with accompanying software tools that allow the reader to directly reproduce the results and employ the computational methods that are presented in the research paper. Some of the issues involved in paradigms for the production, distribution, and use of such reproducible research are discussed.},
	number = {1},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Gentleman, Robert and Lang, Duncan Temple},
	year = {2007},
	keywords = {done},
	pages = {1--23},
	annote = {Publisher: Taylor \& Francis \_eprint: https://doi.org/10.1198/106186007X178663},
}

@article{golub_molecular_1999-3,
	title = {Molecular {Classification} of {Cancer}: {Class} {Discovery} and {Class} {Prediction} by {Gene} {Expression} {Monitoring}},
	volume = {286},
	issn = {0036-8075},
	shorttitle = {Molecular {Classification} of {Cancer}},
	doi = {10.1126/science.286.5439.531},
	abstract = {Although cancer classification has improved over the past 30 years, there has been no general approach for identifying new cancer classes (class discovery) or for assigning tumors to known classes (class prediction). Here, a generic approach to cancer classification based on gene expression monitoring by DNA microarrays is described and applied to human acute leukemias as a test case. A class discovery procedure automatically discovered the distinction between acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL) without previous knowledge of these classes. An automatically derived class predictor was able to determine the class of new leukemia cases. The results demonstrate the feasibility of cancer classification based solely on gene expression monitoring and suggest a general strategy for discovering and predicting cancer classes for other types of cancer, independent of previous biological knowledge.},
	language = {eng},
	number = {5439},
	journal = {Science (New York, N.Y.)},
	author = {Golub, T. R. and Slonim, D. K. and Tamayo, P. and Huard, C. and Gaasenbeek, M. and Mesirov, J. P. and Coller, H. and Loh, M. L. and Downing, J. R. and Caligiuri, M. A. and Bloomfield, C. D. and Lander, E. S.},
	month = oct,
	year = {1999},
	pmid = {10521349},
	keywords = {done},
	pages = {531--537},
}

@article{goodman_what_2016-3,
	title = {What {Does} {Research} {Reproducibility} {Mean}?},
	volume = {8},
	issn = {1946-6234, 1946-6242},
	doi = {10.1126/scitranslmed.aaf5027},
	language = {en},
	number = {341},
	journal = {Science Translational Medicine},
	author = {Goodman, Steven N. and Fanelli, Daniele and Ioannidis, John P. A.},
	month = jun,
	year = {2016},
	keywords = {done},
	pages = {341ps12--341ps12},
}

@book{grolemund_r_nodate-3,
	title = {R for {Data} {Science}},
	abstract = {This book will teach you how to do data science with R: You'll learn how to get your data into R, get it into the most useful structure, transform it, visualise it and model it. In this book, you will find a practicum of skills for data science. Just as a chemist learns how to clean test tubes and stock a lab, you'll learn how to clean data and draw plots— and many other things besides. These are the skills that allow data science to happen, and here you will find the best practices for doing each of these things with R. You'll learn how to use the grammar of graphics, literate programming, and reproducible research to save time. You'll also learn how to manage cognitive resources to facilitate discoveries when wrangling, visualising, and exploring data.},
	author = {Grolemund, Garrett and Wickham, Hadley},
}

@misc{gruber_daring_nodate-6,
	title = {Daring {Fireball}: {Markdown}},
	url = {https://daringfireball.net/projects/markdown/},
	author = {Gruber, John and Swartz, Aron},
	keywords = {done},
}

@misc{gruber_daring_nodate-7,
	title = {Daring {Fireball}: {Markdown}},
	url = {https://daringfireball.net/projects/markdown/},
	author = {Gruber, John and Swartz, Aron},
	keywords = {done},
}

@article{ioannidis_why_2005-3,
	title = {Why {Most} {Published} {Research} {Findings} {Are} {False}},
	volume = {2},
	issn = {1549-1676},
	doi = {10.1371/journal.pmed.0020124},
	abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
	language = {en},
	number = {8},
	journal = {PLOS Medicine},
	author = {Ioannidis, John P. A.},
	month = aug,
	year = {2005},
	keywords = {done},
	pages = {e124},
	annote = {Publisher: Public Library of Science},
}

@article{iyengar_selection_1988-3,
	title = {Selection {Models} and the {File} {Drawer} {Problem}},
	volume = {3},
	issn = {08834237},
	abstract = {Meta-analysis consists of quantitative methods for combining evidence from different studies about a particular issue. A frequent criticism of meta-analysis is that it may be based on a biased sample of all studies that were done. In this paper, we use selection models, or weighted distributions, to deal with one source of bias, namely, the failure to report studies that do not yield statistically significant results. We apply selection models to two approaches that have been suggested for correcting the bias. The fail-safe sample size approach calculates the minimum number of unpublished studies showing nonsignificant results that must have been carried out in order to overturn the conclusion reached from the published studies. The maximum likelihood approach uses a weighted distribution to model the selection bias in the generation of the data and estimates various parameters of interest. We suggest the use of families of weight functions to model plausible biasing mechanisms to study the sensitivity of inferences about effect sizes. By using an example, we show that the maximum likelihood approach has several advantages over the fail-safe sample size approach.},
	number = {1},
	journal = {Statistical Science},
	author = {Iyengar, Satish and Greenhouse, Joel B.},
	year = {1988},
	keywords = {done},
	pages = {109--117},
	annote = {Publisher: Institute of Mathematical Statistics},
}

@article{jasny_again_2011-3,
	title = {Again, and {Again}, and {Again}},
	volume = {334},
	issn = {0036-8075},
	doi = {10.1126/science.334.6060.1225},
	number = {6060},
	journal = {Science},
	author = {Jasny, Barbara R. and Chin, Gilbert and Chong, Lisa and Vignieri, Sacha},
	year = {2011},
	keywords = {done},
	pages = {1225--1225},
	annote = {Publisher: American Association for the Advancement of Science \_eprint: https://science.sciencemag.org/content/334/6060/1225.full.pdf},
}

@article{klein_many_2018-3,
	title = {Many {Labs} 2: {Investigating} {Variation} in {Replicability} across {Samples} and {Settings}},
	volume = {1},
	doi = {10.1177/2515245918810225},
	abstract = {We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p ¡ .05), we found that 15 (54\%) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p ¡ .0001), 14 (50\%) of the replications still provided such evidence, a reflection of the extremely high-powered design. Seven (25\%) of the replications yielded effect sizes larger than the original ones, and 21 (75\%) yielded effect sizes smaller than the original ones. The median comparable Cohen's ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small (¡ 0.20) in 16 of the replications (57\%), and 9 effects (32\%) were in the direction opposite the direction of the original effect. Across settings, the Q statistic indicated significant heterogeneity in 11 (39\%) of the replication effects, and most of those were among the findings with the largest overall effect sizes; only 1 effect that was near zero in the aggregate showed significant heterogeneity according to this measure. Only 1 effect had a tau value greater than .20, an indication of moderate heterogeneity. Eight others had tau values near or slightly above .10, an indication of slight heterogeneity. Moderation tests indicated that very little heterogeneity was attributable to the order in which the tasks were performed or whether the tasks were administered in lab versus online. Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized, rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness scores, respectively). Cumulatively, variability in the observed effect sizes was attributable more to the effect being studied than to the sample or setting in which it was studied.},
	number = {4},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Klein, Richard A. and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G. and Reginald B. Adams, Jr. and Alper, Sinan and Aveyard, Mark and Axt, Jordan R. and Babalola, Mayowa T. and Bahník, Štěpán and Batra, Rishtee and Berkics, Mihály and Bernstein, Michael J. and Berry, Daniel R. and Bialobrzeska, Olga and Binan, Evans Dami and Bocian, Konrad and Brandt, Mark J. and Busching, Robert and Rédei, Anna Cabak and Cai, Huajian and Cambier, Fanny and Cantarero, Katarzyna and Carmichael, Cheryl L. and Ceric, Francisco and Chandler, Jesse and Chang, Jen-Ho and Chatard, Armand and Chen, Eva E. and Cheong, Winnee and Cicero, David C. and Coen, Sharon and Coleman, Jennifer A. and Collisson, Brian and Conway, Morgan A. and Corker, Katherine S. and Curran, Paul G. and Cushman, Fiery and Dagona, Zubairu K. and Dalgar, Ilker and Rosa, Anna Dalla and Davis, William E. and de Bruijn, Maaike and Schutter, Leander De and Devos, Thierry and de Vries, Marieke and Doğulu, Canay and Dozo, Nerisa and Dukes, Kristin Nicole and Dunham, Yarrow and Durrheim, Kevin and Ebersole, Charles R. and Edlund, John E. and Eller, Anja and English, Alexander Scott and Finck, Carolyn and Frankowska, Natalia and Freyre, Miguel-Ángel and Friedman, Mike and Galliani, Elisa Maria and Gandi, Joshua C. and Ghoshal, Tanuka and Giessner, Steffen R. and Gill, Tripat and Gnambs, Timo and Gómez, Ángel and González, Roberto and Graham, Jesse and Grahe, Jon E. and Grahek, Ivan and Green, Eva G. T. and Hai, Kakul and Haigh, Matthew and Haines, Elizabeth L. and Hall, Michael P. and Heffernan, Marie E. and Hicks, Joshua A. and Houdek, Petr and Huntsinger, Jeffrey R. and Huynh, Ho Phi and IJzerman, Hans and Inbar, Yoel and Innes-Ker, Åse H. and Jiménez-Leal, William and John, Melissa-Sue and Joy-Gaba, Jennifer A. and Kamiloğlu, Roza G. and Kappes, Heather Barry and Karabati, Serdar and Karick, Haruna and Keller, Victor N. and Kende, Anna and Kervyn, Nicolas and Knežević, Goran and Kovacs, Carrie and Krueger, Lacy E. and Kurapov, German and Kurtz, Jamie and Lakens, Daniël and Lazarević, Ljiljana B. and Levitan, Carmel A. and Neil A. Lewis, Jr. and Lins, Samuel and Lipsey, Nikolette P. and Losee, Joy E. and Maassen, Esther and Maitner, Angela T. and Malingumu, Winfrida and Mallett, Robyn K. and Marotta, Satia A. and Me{\textbackslash}textbackslashdjedović, Janko and Mena-Pacheco, Fernando and Milfont, Taciano L. and Morris, Wendy L. and Murphy, Sean C. and Myachykov, Andriy and Neave, Nick and Neijenhuijs, Koen and Nelson, Anthony J. and Neto, Félix and Nichols, Austin Lee and Ocampo, Aaron and O'Donnell, Susan L. and Oikawa, Haruka and Oikawa, Masanori and Ong, Elsie and Orosz, Gábor and Osowiecka, Malgorzata and Packard, Grant and Pérez-Sánchez, Rolando and Petrović, Boban and Pilati, Ronaldo and Pinter, Brad and Podesta, Lysandra and Pogge, Gabrielle and Pollmann, Monique M. H. and Rutchick, Abraham M. and Saavedra, Patricio and Saeri, Alexander K. and Salomon, Erika and Schmidt, Kathleen and Schönbrodt, Felix D. and Sekerdej, Maciej B. and Sirlopú, David and Skorinko, Jeanine L. M. and Smith, Michael A. and Smith-Castro, Vanessa and Smolders, Karin C. H. J. and Sobkow, Agata and Sowden, Walter and Spachtholz, Philipp and Srivastava, Manini and Steiner, Troy G. and Stouten, Jeroen and Street, Chris N. H. and Sundfelt, Oskar K. and Szeto, Stephanie and Szumowska, Ewa and Tang, Andrew C. W. and Tanzer, Norbert and Tear, Morgan J. and Theriault, Jordan and Thomae, Manuela and Torres, David and Traczyk, Jakub and Tybur, Joshua M. and Ujhelyi, Adrienn and van Aert, Robbie C. M. and van Assen, Marcel A. L. M. and van der Hulst, Marije and van Lange, Paul A. M. and van 't Veer, Anna Elisabeth and Echeverría, Alejandro Vásquez- and Vaughn, Leigh Ann and Vázquez, Alexandra and Vega, Luis Diego and Verniers, Catherine and Verschoor, Mark and Voermans, Ingrid P. J. and Vranka, Marek A. and Welch, Cheryl and Wichman, Aaron L. and Williams, Lisa A. and Wood, Michael and Woodzicka, Julie A. and Wronska, Marta K. and Young, Liane and Zelenski, John M. and Zhijia, Zeng and Nosek, Brian A.},
	year = {2018},
	keywords = {done},
	pages = {443--490},
	annote = {\_eprint: https://doi.org/10.1177/2515245918810225},
}

@article{knuth_literate_1984-3,
	title = {Literate {Programming}},
	volume = {27},
	issn = {0010-4620},
	doi = {10.1093/comjnl/27.2.97},
	abstract = {The author and his associates have been experimenting for the past several years with a programming language and documentation system called WEB. This paper presents WEB by example, and discusses why the new system appears to be an improvement over previous ones.},
	number = {2},
	journal = {The Computer Journal},
	author = {Knuth, D. E.},
	month = jan,
	year = {1984},
	keywords = {done},
	pages = {97--111},
	annote = {\_eprint: https://academic.oup.com/comjnl/article-pdf/27/2/97/981657/270097.pdf},
}

@book{knuth_texbook_1986-3,
	series = {Computers \& {Typesetting}},
	title = {The {TeXbook}},
	isbn = {978-0-201-13447-6},
	publisher = {Addison-Wesley},
	author = {Knuth, D.E.},
	year = {1986},
	lccn = {85030845},
	keywords = {done},
}

@book{knuth_literate_1992-3,
	title = {Literate {Programming}},
	isbn = {978-0-937073-81-0},
	abstract = {Literate programming is a programming methodology that combines a programming language with a documentation language, making programs more robust, more portable, and more easily maintained than programs written only in a high-level language. Computer programmers already know both kinds of languages; they need only learn a few conventions about alternating between languages to create programs that are works of literature. A literate programmer is an essayist who writes programs for humans to understand, instead of primarily writing instructions for machines to follow. When programs are written in the recommended style they can be transformed into documents by a document compiler and into efficient code by an algebraic compiler. This anthology of essays from the inventor of literate programming includes Knuth's early papers on related topics such as structured programming, as well as the Computer Journal article that launched literate programming itself.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Knuth, Donald E.},
	month = mar,
	year = {1992},
	keywords = {done},
}

@book{lander_r_2017-3,
	address = {Boston},
	edition = {2nd Edition},
	title = {R for {Everyone}: {Advanced} {Analytics} and {Graphics}},
	isbn = {978-0-13-454692-6},
	shorttitle = {R for {Everyone}},
	abstract = {Statistical Computation for Programmers, Scientists, Quants, Excel Users, and Other Professionals Using the open source R language, you can build powerful statistical models to answer many of your most challenging questions. R has traditionally been difficult for non-statisticians to learn, and most R books assume far too much knowledge to be of help. R for Everyone, Second Edition, is the solution. Drawing on his unsurpassed experience teaching new users, professional data scientist Jared P. Lander has written the perfect tutorial for anyone new to statistical programming and modeling. Organized to make learning easy and intuitive, this guide focuses on the 20 percent of R functionality you'll need to accomplish 80 percent of modern data tasks. Lander's self-contained chapters start with the absolute basics, offering extensive hands-on practice and sample code. You'll download and install R; navigate and use the R environment; master basic program control, data import, manipulation, and visualization; and walk through several essential tests. Then, building on this foundation, you'll construct several complete models, both linear and nonlinear, and use some data mining techniques. After all this you'll make your code reproducible with LaTeX, RMarkdown, and Shiny. By the time you're done, you won't just know how to write R programs, you'll be ready to tackle the statistical problems you care about most. Coverage includes Explore R, RStudio, and R packages Use R for math: variable types, vectors, calling functions, and more Exploit data structures, including data.frames, matrices, and lists Read many different types of data Create attractive, intuitive statistical graphics Write user-defined functions Control program flow with if, ifelse, and complex checks Improve program efficiency with group manipulations Combine and reshape multiple datasets Manipulate strings using R's facilities and regular expressions Create normal, binomial, and Poisson probability distributions Build linear, generalized linear, and nonlinear models Program basic statistics: mean, standard deviation, and t-tests Train machine learning models Assess the quality of models and variable selection Prevent overfitting and perform variable selection, using the Elastic Net and Bayesian methods Analyze univariate and multivariate time series data Group data via K-means and hierarchical clustering Prepare reports, slideshows, and web pages with knitr Display interactive data with RMarkdown and htmlwidgets Implement dashboards with Shiny Build reusable R packages with devtools and Rcpp Register your product at informit.com/register for convenient access to downloads, updates, and corrections as they become available.},
	language = {English},
	publisher = {Addison-Wesley Professional},
	author = {Lander, Jared P.},
	month = jun,
	year = {2017},
}

@incollection{leisch_sweave_2002-3,
	address = {Heidelberg},
	title = {Sweave: {Dynamic} {Generation} of {Statistical} {Reports} {Using} {Literate} {Data} {Analysis}},
	isbn = {978-3-7908-1517-7 978-3-642-57489-4},
	shorttitle = {Sweave},
	abstract = {Sweave combines typesetting with LATEX and data anlysis with S into integrated statistical documents. When run through R or Splus, all data analysis output (tables, graphs, . . . ) is created on the fly and inserted into a final LATEX document. Options control which parts of the original S code are shown to or hidden from the reader, respectively. Many S users are also LATEX users, hence no new software has to be learned. The report can be automatically updated if data or analysis change, which allows for truly reproducible research.},
	language = {en},
	booktitle = {Compstat},
	publisher = {Physica-Verlag HD},
	author = {Leisch, Friedrich},
	editor = {Härdle, Wolfgang and Rönz, Bernd},
	year = {2002},
	doi = {10.1007/978-3-642-57489-4_89},
	keywords = {done},
	pages = {575--580},
}

@article{markowetz_five_2015-3,
	title = {Five {Selfish} {Reasons} to {Work} {Reproducibly}},
	volume = {16},
	issn = {1474-760X},
	doi = {10.1186/s13059-015-0850-7},
	language = {en},
	number = {1},
	journal = {Genome Biology},
	author = {Markowetz, Florian},
	month = dec,
	year = {2015},
	pages = {274},
}

@article{mccullough_economics_2008-3,
	title = {Do {Economics} {Journal} {Archives} {Promote} {Replicable} {Research}?},
	volume = {41},
	copyright = {© Canadian Economics Association},
	issn = {1540-5982},
	doi = {10.1111/j.1540-5982.2008.00509.x},
	abstract = {Abstract. All the long-standing archives at economics journals do not facilitate the reproduction of published results. The data-only archives at Journal of Business and Economic Statistics and Economic Journal fail in part because most authors do not contribute data. Results published in the FRB St. Louis Review can rarely be reproduced using the data+code in the journal archive. Recently created archives at top journals should avoid the mistakes of their predecessors. We categorize reasons for archives' failures and identify successful policies.},
	language = {en},
	number = {4},
	journal = {Canadian Journal of Economics/Revue canadienne d'économique},
	author = {McCullough, B. D. and McGeary, Kerry Anne and Harrison, Teresa D.},
	year = {2008},
	keywords = {done},
	pages = {1406--1420},
}

@article{mcnutt_reproducibility_2014-3,
	title = {Reproducibility},
	volume = {343},
	issn = {0036-8075},
	doi = {10.1126/science.1250475},
	number = {6168},
	journal = {Science},
	author = {McNutt, Marcia},
	year = {2014},
	keywords = {done},
	pages = {229--229},
	annote = {Publisher: American Association for the Advancement of Science \_eprint: https://science.sciencemag.org/content/343/6168/229.full.pdf},
}

@article{nosek_promoting_2015-3,
	title = {Promoting an {Open} {Research} {Culture}},
	volume = {348},
	issn = {0036-8075, 1095-9203},
	doi = {10.1126/science.aab2374},
	language = {en},
	number = {6242},
	journal = {Science},
	author = {Nosek, B. A. and Alter, G. and Banks, G. C. and Borsboom, D. and Bowman, S. D. and Breckler, S. J. and Buck, S. and Chambers, C. D. and Chin, G. and Christensen, G. and Contestabile, M. and Dafoe, A. and Eich, E. and Freese, J. and Glennerster, R. and Goroff, D. and Green, D. P. and Hesse, B. and Humphreys, M. and Ishiyama, J. and Karlan, D. and Kraut, A. and Lupia, A. and Mabry, P. and Madon, T. and Malhotra, N. and Mayo-Wilson, E. and McNutt, M. and Miguel, E. and Paluck, E. L. and Simonsohn, U. and Soderberg, C. and Spellman, B. A. and Turitto, J. and VandenBos, G. and Vazire, S. and Wagenmakers, E. J. and Wilson, R. and Yarkoni, T.},
	month = jun,
	year = {2015},
	keywords = {done},
	pages = {1422--1425},
}

@article{nust_opening_2017-3,
	title = {Opening the {Publication} {Process} with {Executable} {Research} {Compendia}},
	volume = {23},
	issn = {1082-9873},
	doi = {10.1045/january2017-nuest},
	language = {en},
	number = {1/2},
	journal = {D-Lib Magazine},
	author = {Nüst, Daniel and Konkol, Markus and Pebesma, Edzer and Kray, Christian and Schutzeichel, Marc and Przibytzin, Holger and Lorenz, Jörg},
	month = jan,
	year = {2017},
}

@article{collaboration_estimating_2015-3,
	title = {Estimating the {Reproducibility} of {Psychological} {Science}},
	volume = {349},
	issn = {0036-8075},
	doi = {10.1126/science.aac4716},
	abstract = {One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.Science, this issue 10.1126/science.aac4716INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P \&lt; .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that we already know this belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
	number = {6251},
	journal = {Science},
	author = {Collaboration, Open Science},
	year = {2015},
	keywords = {done},
	annote = {Publisher: American Association for the Advancement of Science \_eprint: https://science.sciencemag.org/content/349/6251/aac4716.full.pdf},
}

@article{peng_reproducible_2011-3,
	title = {Reproducible {Research} in {Computational} {Science}},
	volume = {334},
	copyright = {Copyright © 2011, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	doi = {10.1126/science.1213847},
	abstract = {Computational science has led to exciting new developments, but the nature of the work has exposed limitations in our ability to evaluate published findings. Reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible.},
	language = {en},
	number = {6060},
	journal = {Science},
	author = {Peng, Roger D.},
	month = dec,
	year = {2011},
	pmid = {22144613},
	keywords = {done},
	pages = {1226--1227},
	annote = {Publisher: American Association for the Advancement of Science Section: Perspective},
}

@article{ram_git_2013-3,
	title = {Git {Can} {Facilitate} {Greater} {Reproducibility} and {Increased} {Transparency} in {Science}},
	volume = {8},
	issn = {1751-0473},
	doi = {10.1186/1751-0473-8-7},
	abstract = {Reproducibility is the hallmark of good science. Maintaining a high degree of transparency in scientific reporting is essential not just for gaining trust and credibility within the scientific community but also for facilitating the development of new ideas. Sharing data and computer code associated with publications is becoming increasingly common, motivated partly in response to data deposition requirements from journals and mandates from funders. Despite this increase in transparency, it is still difficult to reproduce or build upon the findings of most scientific publications without access to a more complete workflow.},
	number = {1},
	journal = {Source Code for Biology and Medicine},
	author = {Ram, Karthik},
	month = feb,
	year = {2013},
	pages = {7},
}

@misc{ramsey_noweb_nodate-3,
	title = {Noweb {Home} {Page}},
	url = {https://www.cs.tufts.edu/\textasciitilde nr/noweb/},
	author = {Ramsey, Norman},
	keywords = {done},
}

@book{r_core_team_r_2020-3,
	address = {Vienna, Austria},
	title = {R: {A} {Language} and {Environment} for {Statistical} {Computing}},
	publisher = {R Foundation for Statistical Computing},
	author = {{R Core Team}},
	year = {2020},
	keywords = {done},
}

@book{riederer_r_nodate-3,
	title = {R {Markdown} {Cookbook}},
	abstract = {This book showcases short, practical examples of lesser-known tips and tricks to helps users get the most out of these tools. After reading this book, you will understand how R Markdown documents are transformed from plain text and how you may customize nearly every step of this processing. For example, you will learn how to dynamically create content from R code, reference code in other documents or chunks, control the formatting with customer templates, fine-tune how your code is processed, and incorporate multiple languages into your analysis.},
	author = {Riederer, Christophe Dervieux, Emily, Yihui Xie},
	keywords = {done},
}

@inproceedings{rosenthal_file_1979-3,
	title = {The {File} {Drawer} {Problem} and {Tolerance} for {Null} {Results}.},
	volume = {86},
	publisher = {Psychological Bulletin},
	author = {Rosenthal, R.},
	year = {1979},
	keywords = {done},
	pages = {638--641},
}

@book{rstudio_team_rstudio_2020-3,
	address = {Boston, MA},
	title = {{RStudio}: {Integrated} {Development} {Environment} for r},
	publisher = {RStudio, PBC.},
	author = {{RStudio Team}},
	year = {2020},
}

@article{schwab_reproducible_1995-3,
	title = {Reproducible {Electronic} {Documents}},
	abstract = {To organize computational scientific research and hence to conveniently transfer our technology, we impose a simple filing discipline on the authors in our laboratory. A document's makefile includes laboratory-wide standard rules that offer readers these four standard commands: make burn removes the document's result figures, make build recomputes them, make view displays the figures, and make clean removes any intermediate files. Although we developed these standards to aid readers we discovered that authors are often the principal beneficiaries.},
	language = {en},
	author = {Schwab, Matthias and Karrenbach, Martin and Claerbout, Jon},
	year = {1995},
	keywords = {done},
	pages = {14},
}

@article{simmons_false-positive_2011-3,
	title = {False-{Positive} {Psychology}: {Undisclosed} {Flexibility} in {Data} {Collection} and {Analysis} {Allows} {Presenting} {Anything} as {Significant}},
	volume = {22},
	doi = {10.1177/0956797611417632},
	abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings (\${\textbackslash}textbackslashleq\$ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
	number = {11},
	journal = {Psychological Science},
	author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
	year = {2011},
	pmid = {22006061},
	keywords = {done},
	pages = {1359--1366},
	annote = {\_eprint: https://doi.org/10.1177/0956797611417632},
}

@book{tierney_rmarkdown_nodate-3,
	title = {{RMarkdown} for {Scientists}},
	abstract = {A book created for a 3 hour workshop on rmarkdown},
	author = {Tierney, Nicholas},
	keywords = {done},
}

@book{wickham_r_2016-3,
	address = {Beijing},
	title = {R for {Data} {Science}: {Import}, {Tidy}, {Transform}, {Visualize}, and {Model} {Data}},
	isbn = {978-1-4919-1039-9},
	shorttitle = {R for {Data} {Science}},
	language = {eng},
	publisher = {O'Reilly},
	author = {Wickham, Hadley and Grolemund, Garrett},
	year = {2016},
	lccn = {006.312 Wic, 519.2 W63r, 006.312 WIC},
}

@article{wikipedia_meta-analysis_2020-3,
	title = {Meta-{Analysis}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	abstract = {A meta-analysis is a statistical analysis that combines the results of multiple scientific studies. Meta-analysis can be performed when there are multiple scientific studies addressing the same question, with each individual study reporting measurements that are expected to have some degree of error. The aim then is to use approaches from statistics to derive a pooled estimate closest to the unknown common truth based on how this error is perceived. Existing methods for meta-analysis yield a weighted average from the results of the individual studies, and what differs is the manner in which these weights are allocated and also the manner in which the uncertainty is computed around the point estimate thus generated. In addition to providing an estimate of the unknown common truth, meta-analysis has the capacity to contrast results from different studies and identify patterns among study results, sources of disagreement among those results, or other interesting relationships that may come to light in the context of multiple studies.A key benefit of this approach is the aggregation of information leading to a higher statistical power and more robust point estimate than is possible from the measure derived from any individual study. However, in performing a meta-analysis, an investigator must make choices which can affect the results, including deciding how to search for studies, selecting studies based on a set of objective criteria, dealing with incomplete data, analyzing the data, and accounting for or choosing not to account for publication bias. Judgment calls made in completing a meta-analysis may affect the results. For example, Wanous and colleagues examined four pairs of meta-analyses on the four topics of (a) job performance and satisfaction relationship, (b) realistic job previews, (c) correlates of role conflict and ambiguity, and (d) the job satisfaction and absenteeism relationship, and illustrated how various judgement calls made by the researchers produced different results.Meta-analyses are often, but not always, important components of a systematic review procedure. For instance, a meta-analysis may be conducted on several clinical trials of a medical treatment, in an effort to obtain a better understanding of how well the treatment works. Here it is convenient to follow the terminology used by the Cochrane Collaboration, and use "meta-analysis" to refer to statistical methods of combining evidence, leaving other aspects of 'research synthesis' or 'evidence synthesis', such as combining information from qualitative studies, for the more general context of systematic reviews. A meta-analysis is a secondary source.},
	language = {en},
	journal = {Wikipedia},
	author = {{wikipedia}},
	month = aug,
	year = {2020},
	keywords = {done},
}

@incollection{xie_knitr_2014-3,
	title = {Knitr: {A} {Comprehensive} {Tool} for {Reproducible} {Research} in {R}},
	booktitle = {Implementing {Reproducible} {Computational} {Research}},
	publisher = {Chapman and Hall/CRC},
	author = {Xie, Yihui},
	editor = {Stodden, Victoria and Leisch, Friedrich and Peng, Roger D.},
	year = {2014},
	keywords = {done},
	annote = {ISBN 978-1466561595},
}

@book{xie_dynamic_2015-3,
	address = {Boca Raton, Florida},
	edition = {Second},
	title = {Dynamic {Documents} with {R} and {Knitr}},
	publisher = {Chapman and Hall/CRC},
	author = {Xie, Yihui},
	year = {2015},
	keywords = {done},
	annote = {ISBN 978-1498716963},
}

@book{xie_r_2018-3,
	address = {Boca Raton, Florida},
	title = {R {Markdown}: {The} {Definitive} {Guide}},
	publisher = {Chapman and Hall/CRC},
	author = {Xie, Yihui and Allaire, J.J. and Grolemund, Garrett},
	year = {2018},
	keywords = {done},
	annote = {ISBN 9781138359338},
}

@book{xie_knitr_2020-3,
	title = {Knitr: {A} {General}-{Purpose} {Package} for {Dynamic} {Report} {Generation} in r},
	author = {Xie, Yihui},
	year = {2020},
	keywords = {done},
	annote = {R package version 1.29},
	annote = {Type: Manual},
}

@article{young_why_2008-3,
	title = {Why {Current} {Publication} {Practices} {May} {Distort} {Science}},
	volume = {5},
	issn = {1549-1676},
	doi = {10.1371/journal.pmed.0050201},
	abstract = {The current system of publication in biomedical research provides a distorted view of the reality of scientific data that are generated in the laboratory and clinic. This system can be studied by applying principles from the field of economics. The “winner's curse,” a more general statement of publication bias, suggests that the small proportion of results chosen for publication are unrepresentative of scientists' repeated samplings of the real world. The self-correcting mechanism in science is retarded by the extreme imbalance between the abundance of supply (the output of basic science laboratories and clinical investigations) and the increasingly limited venues for publication (journals with sufficiently high impact). This system would be expected intrinsically to lead to the misallocation of resources. The scarcity of available outlets is artificial, based on the costs of printing in an electronic age and a belief that selectivity is equivalent to quality. Science is subject to great uncertainty: we cannot be confident now which efforts will ultimately yield worthwhile achievements. However, the current system abdicates to a small number of intermediates an authoritative prescience to anticipate a highly unpredictable future. In considering society's expectations and our own goals as scientists, we believe that there is a moral imperative to reconsider how scientific data are judged and disseminated.},
	language = {en},
	number = {10},
	journal = {PLoS Medicine},
	author = {Young, Neal S and Ioannidis, John P. A and Al-Ubaydli, Omar},
	month = oct,
	year = {2008},
	keywords = {done},
	pages = {e201},
}

@misc{noauthor_dear_nodate-6,
	title = {Dear {Colleague} {Letter}: {Robust} and {Reliable} {Research} in the {Social}, {Behavioral}, and {Economic} {Sciences} ({Nsf16137}) {\textbackslash}textbar {NSF} - {National} {Science} {Foundation}},
	url = {https://www.nsf.gov/pubs/2016/nsf16137/nsf16137.jsp},
	keywords = {done},
}

@misc{noauthor_dear_nodate-7,
	title = {Dear {Colleague} {Letter}: {Robust} and {Reliable} {Research} in the {Social}, {Behavioral}, and {Economic} {Sciences} ({Nsf16137}) {\textbackslash}textbar {NSF} - {National} {Science} {Foundation}},
	url = {https://www.nsf.gov/pubs/2016/nsf16137/nsf16137.jsp},
	keywords = {done},
}

@misc{noauthor_american_nodate-3,
	title = {American {Economic} {Association}},
	url = {https://www.aeaweb.org/journals/data},
}

@misc{noauthor_pandoc_nodate-3,
	title = {Pandoc - {Pandoc} {User}'s {Guide}},
	url = {https://pandoc.org/MANUAL.html},
}

@article{nosek_estimating_2015-3,
	title = {Estimating the {Reproducibility} of {Psychological} {Science}},
	volume = {349},
	issn = {0036-8075},
	doi = {10.1126/science.aac4716},
	abstract = {One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.Science, this issue 10.1126/science.aac4716INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P \&lt; .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that we already know this belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
	number = {6251},
	journal = {Science},
	author = {Nosek, Brian A. and {et al}},
	year = {2015},
	keywords = {done},
	annote = {Publisher: American Association for the Advancement of Science \_eprint: https://science.sciencemag.org/content/349/6251/aac4716.full.pdf},
}

@misc{saetrevik_replikasjonskrisen_2017-1,
	title = {Replikasjonskrisen},
	url = {https://psykologtidsskriftet.no/fagessay/2017/07/replikasjonskrisen},
	language = {Norsk},
	author = {Sætrevik, B.},
	year = {2017},
}

@article{leek_opinion_nodate-1,
	title = {Opinion: {Reproducible} research can still be wrong: {Adopting} a prevention approach.},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Leek, J. T and Peng, R. D.},
	note = {Edition: 112
Section: 6},
	pages = {1645--1646},
}

@article{noauthor_notitle_nodate-1,
}

@article{schmidt_reproducible_2015-1,
	title = {Reproducible {Research} {Using} {RMarkdown} and {Git} through {Rstudio}},
	url = {https://rpubs.com/marschmi/105639>},
	journal = {RPubs by Rstudio},
	author = {Schmidt, M. L.},
	year = {2015},
}

@techreport{bollen_social_2015-4,
	title = {Social, {Behavioral}, and {Economic} {Sciences} {Perspectives} on {Robust} and {Reliable} {Science}},
	number = {Report of the Subcommittee on Replicability in Science Advisory Committee to the National Science Foundation Directorate for Social, Behavioral, and Economic Sciences},
	institution = {NSF},
	author = {Bollen, Kenneth and Cacioppo, John T. and Krosnick, Jon A. and Olds, James L. and Kaplan, Robert M.},
	year = {2015},
	keywords = {done},
}

@article{gentleman_statistical_2007-4,
	title = {Statistical {Analyses} and {Reproducible} {Research}},
	volume = {16},
	doi = {10.1198/106186007X178663},
	abstract = {It is important, if not essential, to integrate the computations and code used in data analyses, methodological descriptions, simulations, and so on with the documents that describe and rely on them. This integration allows readers to both verify and adapt the claims in the documents. Authors can easily reproduce the results in the future, and they can present the document's contents in a different medium, for example, with interactive controls. This article describes a software framework for both authoring and distributing these integrated, dynamic documents that contain text, code, data, and any auxiliary content needed to recreate the computations. The documents are dynamic in that the contents— including figures, tables, and so on— can be recalculated each time a view of the document is generated. Our model treats a dynamic document as a master or “source” document from which one can generate different views in the form of traditional, derived documents for different audiences.We introduce the concept of a compendium as a container for one or more dynamic documents and the different elements needed when processing them, such as code and data. The compendium serves as a means for distributing, managing, and updating the collection.The step from disseminating analyses via a compendium to reproducible research is a small one. By reproducible research, we mean research papers with accompanying software tools that allow the reader to directly reproduce the results and employ the computational methods that are presented in the research paper. Some of the issues involved in paradigms for the production, distribution, and use of such reproducible research are discussed.},
	number = {1},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Gentleman, Robert and Lang, Duncan Temple},
	year = {2007},
	keywords = {done},
	pages = {1--23},
	annote = {Publisher: Taylor \& Francis \_eprint: https://doi.org/10.1198/106186007X178663},
}

@article{goodman_what_2016-4,
	title = {What {Does} {Research} {Reproducibility} {Mean}?},
	volume = {8},
	issn = {1946-6234, 1946-6242},
	doi = {10.1126/scitranslmed.aaf5027},
	language = {en},
	number = {341},
	journal = {Science Translational Medicine},
	author = {Goodman, Steven N. and Fanelli, Daniele and Ioannidis, John P. A.},
	month = jun,
	year = {2016},
	keywords = {done},
	pages = {341ps12--341ps12},
}

@article{markowetz_five_2015-4,
	title = {Five {Selfish} {Reasons} to {Work} {Reproducibly}},
	volume = {16},
	issn = {1474-760X},
	doi = {10.1186/s13059-015-0850-7},
	language = {en},
	number = {1},
	journal = {Genome Biology},
	author = {Markowetz, Florian},
	month = dec,
	year = {2015},
	pages = {274},
}

@article{young_why_2008-4,
	title = {Why {Current} {Publication} {Practices} {May} {Distort} {Science}},
	volume = {5},
	issn = {1549-1676},
	doi = {10.1371/journal.pmed.0050201},
	abstract = {The current system of publication in biomedical research provides a distorted view of the reality of scientific data that are generated in the laboratory and clinic. This system can be studied by applying principles from the field of economics. The “winner's curse,” a more general statement of publication bias, suggests that the small proportion of results chosen for publication are unrepresentative of scientists' repeated samplings of the real world. The self-correcting mechanism in science is retarded by the extreme imbalance between the abundance of supply (the output of basic science laboratories and clinical investigations) and the increasingly limited venues for publication (journals with sufficiently high impact). This system would be expected intrinsically to lead to the misallocation of resources. The scarcity of available outlets is artificial, based on the costs of printing in an electronic age and a belief that selectivity is equivalent to quality. Science is subject to great uncertainty: we cannot be confident now which efforts will ultimately yield worthwhile achievements. However, the current system abdicates to a small number of intermediates an authoritative prescience to anticipate a highly unpredictable future. In considering society's expectations and our own goals as scientists, we believe that there is a moral imperative to reconsider how scientific data are judged and disseminated.},
	language = {en},
	number = {10},
	journal = {PLoS Medicine},
	author = {Young, Neal S and Ioannidis, John P. A and Al-Ubaydli, Omar},
	month = oct,
	year = {2008},
	keywords = {done},
	pages = {e201},
}

@misc{saetrevik_replikasjonskrisen_2017-2,
	title = {Replikasjonskrisen},
	url = {https://psykologtidsskriftet.no/fagessay/2017/07/replikasjonskrisen},
	language = {Norsk},
	author = {Sætrevik, B.},
	year = {2017},
}

@article{leek_opinion_nodate-2,
	title = {Opinion: {Reproducible} research can still be wrong: {Adopting} a prevention approach.},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Leek, J. T and Peng, R. D.},
	note = {Edition: 112
Section: 6},
	pages = {1645--1646},
}

@article{schmidt_reproducible_2015-2,
	title = {Reproducible {Research} {Using} {RMarkdown} and {Git} through {Rstudio}},
	url = {https://rpubs.com/marschmi/105639},
	journal = {RPubs by Rstudio},
	author = {Schmidt, M. L.},
	year = {2015},
}

@article{kvisvik_sammenhengen_2018,
	title = {Sammenhengen mellom høyde og inntekt},
	url = {http://kvisvikconsulting.no/sammenhengen-mellom-hoyde-og-inntekt/},
	author = {Kvisvik, Eli},
	year = {2018},
}

@article{case_stature_nodate,
	title = {Stature and status: {Height}, ability, and labor market outcomes},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2709415/},
	author = {Case, Anne and Paxson, Christina},
}

@article{judge_effect_2004,
	title = {The {Effect} of {Physical} {Height} on {Workplace} {Success} and {Income}: {Preliminary} {Test} of a {Theoretical} {Model}},
	volume = {89},
	number = {3},
	journal = {Journal of Applied Psychology},
	author = {Judge, Timothy A. and Cable, Daniel M.},
	year = {2004},
	pages = {428--441},
}

@article{judge2004,
	title = {The Effect of Physical Height on Workplace Success and Income: Preliminary Test of a Theoretical Model.},
	author = {Judge, Timothy A. and Cable, Daniel M.},
	year = {2004},
	date = {2004},
	journal = {Journal of Applied Psychology},
	pages = {428--441},
	volume = {89},
	number = {3},
	doi = {10.1037/0021-9010.89.3.428},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0021-9010.89.3.428},
	langid = {en}
}
